{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before you turn this problem in, make sure everything runs as expected. First, **restart the kernel** (in the menubar, select Kernel$\\rightarrow$Restart) and then **run all cells** (in the menubar, select Cell$\\rightarrow$Run All).\n",
    "\n",
    "Make sure you fill in any place that says `YOUR CODE HERE` or \"YOUR ANSWER HERE\".\n",
    "\n",
    "__Also, please write how much time it took you to finish the homework.__ This will not affect your grade in any way and is used for statistical purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "TIME_SPENT = \"01h45m\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d57d959511cb57a8ca6c9ed651272edd",
     "grade": false,
     "grade_id": "cell-76ed044166f5efdd",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Homework 1\n",
    "### NLP Basics & NLP Pipelines\n",
    "\n",
    "Welcome to Homework 1! \n",
    "\n",
    "The homework contains several tasks. You can find the amount of points that you get for the correct solution in the task header. Maximum amount of points for each homework is _four_.\n",
    "\n",
    "The **grading** for each task is the following:\n",
    "- correct answer - **full points**\n",
    "- insufficient solution or solution resulting in the incorrect output - **half points**\n",
    "- no answer or completely wrong solution - **no points**\n",
    "\n",
    "Even if you don't know how to solve the task, we encourage you to write down your thoughts and progress and try to address the issues that stop you from completing the task.\n",
    "\n",
    "When working on the written tasks, try to make your answers short and accurate. Most of the times, it is possible to answer the question in 1-3 sentences.\n",
    "\n",
    "When writing code, make it readable. Choose appropriate names for your variables (`a = 'cat'` - not good, `word = 'cat'` - good). Avoid constructing lines of code longer than 100 characters (79 characters is ideal). If needed, provide the commentaries for your code, however, a good code should be easily readable without them :)\n",
    "\n",
    "Finally, all your answers should be written only by yourself. If you copy them from other sources it will be considered as an academic fraud. You can discuss the tasks with your classmates but each solution must be individual.\n",
    "\n",
    "<font color='red'>**Important!:**</font> **before sending your solution, do the `Kernel -> Restart & Run All` to ensure that all your code works.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (1.26.7) or chardet (3.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize, sent_tokenize, pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "\n",
    "import re\n",
    "from collections import defaultdict, Counter\n",
    "import requests\n",
    "from string import punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the text\n",
    "# The Project Gutenberg eBook of The Adventures of Sherlock Holmes, by Arthur Conan Doyle\n",
    "url = 'https://www.gutenberg.org/files/1661/1661-0.txt'\n",
    "raw_text = requests.get(url).content.decode('utf-8')\n",
    "\n",
    "# Remove the Gutenberg metadata\n",
    "text_start = \"*** START OF THE PROJECT GUTENBERG EBOOK THE ADVENTURES OF SHERLOCK HOLMES ***\"\n",
    "text_end = \"*** END OF THE PROJECT GUTENBERG EBOOK THE ADVENTURES OF SHERLOCK HOLMES ***\"\n",
    "raw_text = raw_text.split(text_start)[1].split(text_end)[0].strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1. Tokenize and count statistics (1 point)\n",
    "\n",
    "Using either NLTK tools, tokenize your text data.\n",
    "\n",
    "Compute and output the following:\n",
    "- number of sentences \n",
    "- number of tokens \n",
    "- number of unique tokens (or types)\n",
    "- average length of a sentence\n",
    "- average length of a token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "65a168d540196eeeb08ad3743224e0f3",
     "grade": false,
     "grade_id": "cell-d503025fa37eb2b8",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences: 4716\n",
      "Number of tokens: 111710\n",
      "Number of unique tokens (or types): 9638\n",
      "Average sentence length: 23.687446988973708\n",
      "Average token length: 4.070459224778444\n"
     ]
    }
   ],
   "source": [
    "# Please, use these variables\n",
    "num_sentences = 0\n",
    "num_tokens = 0\n",
    "num_unique_tokens = 0\n",
    "avg_sentence_len = 0\n",
    "avg_token_len = 0\n",
    "\n",
    "# YOUR CODE STARTS HERE\n",
    "num_sentences = len(sent_tokenize(raw_text))\n",
    "num_tokens = len([word for word in word_tokenize(raw_text) if word not in punctuation])\n",
    "num_unique_tokens = len(set([word for word in word_tokenize(raw_text) if word not in punctuation]))\n",
    "avg_sentence_len = num_tokens/num_sentences\n",
    "avg_token_len = sum(len(word) for word in word_tokenize(raw_text)) / num_tokens\n",
    "# YOUR CODE ENDS HERE\n",
    "\n",
    "print(\"Number of sentences:\", num_sentences)\n",
    "print(\"Number of tokens:\", num_tokens)\n",
    "print(\"Number of unique tokens (or types):\", num_unique_tokens)\n",
    "print(\"Average sentence length:\", avg_sentence_len)\n",
    "print(\"Average token length:\", avg_token_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "41a42100874743d5cdd5b784174dba18",
     "grade": true,
     "grade_id": "task-1",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2. Lemmatization and normalization (1 point)\n",
    "\n",
    "Using NTLK, lemmatize your data.\n",
    "Make a copy of your data but this time transform all the tokens and lemmas into the lowercase.\n",
    "\n",
    "Provide the following statistics:\n",
    "- Number of unique lemmas (original case)\n",
    "- Number of unique lemmas (lower case)\n",
    "- Number of unique tokens (original case)\n",
    "- Number of unique tokens (lower case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d17774d377fd34a7764476e2c260ac16",
     "grade": false,
     "grade_id": "cell-a51db4fc6d067dd3",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/utlab/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to /home/utlab/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/utlab/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique lemmas (original case): 8103\n",
      "Number of unique lemmas (lower case): 7564\n",
      "Number of unique tokens (original case): 9638\n",
      "Number of unique tokens (lower case): 9049\n"
     ]
    }
   ],
   "source": [
    "def tagset_map(tag):\n",
    "    tag = re.sub('^N[A-Z]{1,3}$', 'n', tag)\n",
    "    tag = re.sub('^J[A-Z]{1,2}$', 'a', tag)\n",
    "    tag = re.sub('^R[A-Z]{1,2}$', 'r', tag)\n",
    "    tag = re.sub('^V[A-Z]{1,2}$', 'v', tag)\n",
    "    if tag not in list('narv'):\n",
    "        tag = 'n'\n",
    "    return tag\n",
    "\n",
    "\n",
    "# Lemmatize your data\n",
    "# YOUR CODE STARTS HERE\n",
    "from nltk import pos_tag_sents\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "def lemmatize(sents):\n",
    "    lemmas = []\n",
    "    for sent in pos_tag_sents(sents):\n",
    "        for token, tag in sent:\n",
    "            word_tag = tagset_map(tag)\n",
    "            lemma = lemmatizer.lemmatize(token, pos=word_tag)\n",
    "            lemmas.append(lemma)\n",
    "\n",
    "    return lemmas\n",
    "\n",
    "\n",
    "sent_raw = [word_tokenize(s) for s in sent_tokenize(raw_text)]\n",
    "lemmatized_sents = lemmatize(sent_raw)\n",
    "# YOUR CODE ENDS HERE\n",
    "\n",
    "\n",
    "# Make a copy of your tokens but in lowercase\n",
    "# YOUR CODE STARTS HERE\n",
    "lemmatized_sents_lowercase = [word.lower() for word in lemmatized_sents if word not in punctuation]\n",
    "tokens_lowercase = [word.lower() for word in word_tokenize(raw_text) if word not in punctuation]\n",
    "# YOUR CODE ENDS HERE\n",
    "\n",
    "\n",
    "# Count statistics (no need to calculate the number of unique tokens in original case since we did it in Task 2)\n",
    "# Please, use these variables\n",
    "num_unique_lemmas = 0\n",
    "num_unique_lemmas_lower = 0\n",
    "num_unique_tokens_lower = 0\n",
    "\n",
    "# YOUR CODE STARTS HERE\n",
    "num_unique_lemmas = len(set([lemma for lemma in lemmatized_sents if lemma not in punctuation]))\n",
    "num_unique_lemmas_lower = len(set(lemmatized_sents_lowercase))\n",
    "num_unique_tokens_lower = len(set(tokens_lowercase))\n",
    "# YOUR CODE ENDS HERE\n",
    "\n",
    "# Print out the numbers\n",
    "print(\"Number of unique lemmas (original case):\", num_unique_lemmas)\n",
    "print(\"Number of unique lemmas (lower case):\", num_unique_lemmas_lower)\n",
    "print(\"Number of unique tokens (original case):\", num_unique_tokens)\n",
    "print(\"Number of unique tokens (lower case):\", num_unique_tokens_lower)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d440cb25256ebc68bc5f3cf0c88de098",
     "grade": true,
     "grade_id": "task-2",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3. Preprocessing function (0.5 points)\n",
    "\n",
    "To make preprocessing easier in the future, wrap everything into a function that takes raw text as input and outputs tokens and lemmas. The function will also have special arguments for removing stopwords, punctuation, and lowercasing the outputs.\n",
    "\n",
    "__NB__: NLTK morphological analyzer takes word context into account, so you might want to assign pos tags to the tokens before normalization.\n",
    "\n",
    "Tip: This book has some punctuation characters that are not present in Python's `punctuation` variable. You might want to return to this task after looking at the results of the next one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0e68a976d8db18968963bc521c258c92",
     "grade": false,
     "grade_id": "cell-9147f6f734cc0554",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/utlab/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "def preprocess(raw_text, remove_stopwords=True, remove_punctuation=True, lowercase=True):\n",
    "    \"\"\"Preprocess raw text.\n",
    "    \n",
    "    Args:\n",
    "        raw_text (str): Text to preprocess.\n",
    "        remove_stopwords (bool, optional): Whether to remove the stopwords or not.\n",
    "        remove_punctuation (bool, optional): Whether to remove the punctuation or not.\n",
    "        lowercase (bool, optional): Lowercase all the tokens.\n",
    "        \n",
    "    Returns:\n",
    "        tokens (list[str]): List of tokens from the text.\n",
    "        lemmas (list[str]): List of lemmas from the text.\n",
    "    \n",
    "    \"\"\"\n",
    "    # YOUR CODE STARTS HERE\n",
    "    tokens = []\n",
    "    lemmas = []\n",
    "\n",
    "    tokens = [word_tokenize(s) for s in sent_tokenize(raw_text)]\n",
    "    \n",
    "    if lowercase:\n",
    "        tmp_tokens=[]\n",
    "        for token in tokens:\n",
    "            lower_token = [word.lower() for word in token]\n",
    "            tmp_tokens.append(lower_token)\n",
    "        tokens=tmp_tokens\n",
    "    if remove_stopwords:\n",
    "        tmp_tokens=[]\n",
    "        nltk_stopwords = set(stopwords.words('english'))\n",
    "        for token in tokens:\n",
    "            stop_token = [word for word in token if word not in nltk_stopwords]\n",
    "            tmp_tokens.append(stop_token)\n",
    "        tokens=tmp_tokens\n",
    "    if remove_punctuation:\n",
    "        tmp_tokens=[]\n",
    "        for token in tokens:\n",
    "            pun_token = [word for word in token if word not in punctuation]\n",
    "            tmp_tokens.append(pun_token)\n",
    "        tokens=tmp_tokens\n",
    "    for word in pos_tag_sents(tokens):\n",
    "        tmp_tokens=[]\n",
    "        for token, tag in word:\n",
    "            word_tag = tagset_map(tag)\n",
    "            lemma = lemmatizer.lemmatize(token, pos=word_tag)\n",
    "            lemmas.append(lemma)\n",
    "    # YOUR CODE ENDS HERE\n",
    "\n",
    "    return tokens, lemmas\n",
    "\n",
    "\n",
    "tokens, lemmas=preprocess(raw_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4. Splitting the text into chapters (1 point)\n",
    "\n",
    "The Adventures of Sherlock Holmes has twelve adventures. If you look at the text (https://www.gutenberg.org/files/1661/1661-0.txt) each of them starts with at title, e.g. \"I. A SCANDAL IN BOHEMIA\" or \"II. THE RED-HEADED LEAGUE\".\n",
    "\n",
    "Look through the text and come up with a regular expression that only captures the titles. Write all the titles in order using `re.findall`. Then, split the text into twelve adventures with `re.split`. Finally, join the titles with the corresponding texts in an ordered dict or a list of tuples.\n",
    "\n",
    "Tip: https://regex101.com/ is a great website to test your regular expressions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "701b7f5dfd30218e7e2f3b6a3c358042",
     "grade": false,
     "grade_id": "cell-740f24ed789af437",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE STARTS HERE\n",
    "def get_book_dict(raw_text):\n",
    "    pattern = r'([IVX]+\\.\\s(?:[A-Z]+(\\s|\\-|\\’)){2,})'\n",
    "\n",
    "    find_all_titles = re.findall(pattern, raw_text)\n",
    "    tittles = [tittle[0] for tittle in find_all_titles]\n",
    "    book_dict = {}\n",
    "\n",
    "    tmp_raw_text = raw_text\n",
    "    for tittle in reversed(tittles):\n",
    "        split_sents = re.split(tittle, tmp_raw_text)\n",
    "        tmp_raw_text = split_sents[0]\n",
    "        book_dict[tittle] = split_sents[1]\n",
    "\n",
    "    return dict(reversed(list(book_dict.items())))\n",
    "# YOUR CODE ENDS HERE\n",
    "\n",
    "print(len(get_book_dict(raw_text)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 5. Statistics by chapter (0.5 points)\n",
    "\n",
    "Using your `preprocess` function from the Task 3, for each adventure, print out the following information:\n",
    "- Title\n",
    "- Number of tokens\n",
    "- Number of unique words\n",
    "- Number of unique lemmas\n",
    "- Top 20 lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "65c92ad100c278373e322b5657a5adaa",
     "grade": false,
     "grade_id": "cell-8ec5b069710c496b",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: I. A SCANDAL IN BOHEMIA\n",
      "Number of tokens: 4437\n",
      "Number of unique words: 1909\n",
      "Number of unique lemmas: 4437\n",
      "Top 20 lemmas: [('“', 283), ('”', 268), ('’', 61), ('holmes', 46), ('say', 39), ('one', 27), ('upon', 25), ('could', 25), ('come', 23), ('see', 22), ('know', 22), ('man', 21), ('may', 21), ('‘', 21), ('take', 19), ('would', 19), ('make', 19), ('street', 18), ('king', 18), ('majesty', 18)]\n",
      "------------------------------------------------------------\n",
      "Title: II. THE RED-HEADED LEAGUE\n",
      "Number of tokens: 4754\n",
      "Number of unique words: 1880\n",
      "Number of unique lemmas: 4754\n",
      "Top 20 lemmas: [('“', 245), ('”', 190), ('’', 138), ('say', 66), ('‘', 62), ('mr.', 54), ('holmes', 53), ('upon', 49), ('would', 35), ('come', 31), ('one', 30), ('well', 27), ('see', 26), ('little', 25), ('man', 25), ('could', 23), ('wilson', 22), ('go', 22), ('time', 19), ('take', 19)]\n",
      "------------------------------------------------------------\n",
      "Title: III. A CASE OF IDENTITY\n",
      "Number of tokens: 3568\n",
      "Number of unique words: 1523\n",
      "Number of unique lemmas: 3568\n",
      "Top 20 lemmas: [('“', 164), ('”', 164), ('’', 61), ('say', 56), ('mr.', 47), ('holmes', 46), ('would', 39), ('upon', 35), ('come', 33), ('little', 28), ('could', 24), ('hosmer', 22), ('see', 20), ('one', 19), ('windibank', 19), ('man', 18), ('go', 18), ('father', 17), ('make', 16), ('never', 16)]\n",
      "------------------------------------------------------------\n",
      "Title: IV. THE BOSCOMBE VALLEY MYSTERY\n",
      "Number of tokens: 4779\n",
      "Number of unique words: 1877\n",
      "Number of unique lemmas: 4779\n",
      "Top 20 lemmas: [('“', 247), ('”', 206), ('’', 66), ('say', 52), ('holmes', 46), ('man', 43), ('upon', 42), ('see', 39), ('know', 38), ('mccarthy', 37), ('father', 33), ('one', 31), ('could', 29), ('come', 27), ('would', 27), ('son', 27), ('go', 26), ('little', 25), ('shall', 24), ('young', 24)]\n",
      "------------------------------------------------------------\n",
      "Title: V. THE FIVE ORANGE PIPS\n",
      "Number of tokens: 3748\n",
      "Number of unique words: 1727\n",
      "Number of unique lemmas: 3748\n",
      "Top 20 lemmas: [('“', 195), ('”', 160), ('’', 65), ('upon', 47), ('say', 36), ('‘', 36), ('come', 33), ('one', 29), ('paper', 26), ('holmes', 25), ('take', 25), ('may', 24), ('would', 20), ('man', 20), ('see', 20), ('day', 18), ('must', 18), ('time', 18), ('letter', 18), ('however', 16)]\n",
      "------------------------------------------------------------\n",
      "Title: VI. THE MAN WITH THE TWISTED LIP\n",
      "Number of tokens: 4698\n",
      "Number of unique words: 1944\n",
      "Number of unique lemmas: 4698\n",
      "Top 20 lemmas: [('“', 236), ('”', 223), ('’', 60), ('upon', 54), ('one', 35), ('say', 34), ('see', 30), ('man', 30), ('come', 29), ('st.', 28), ('holmes', 28), ('make', 27), ('could', 27), ('find', 27), ('clair', 27), ('know', 26), ('would', 25), ('little', 21), ('room', 21), ('take', 21)]\n",
      "------------------------------------------------------------\n",
      "Title: VII. THE ADVENTURE OF THE BLUE CARBUNCLE\n",
      "Number of tokens: 4128\n",
      "Number of unique words: 1723\n",
      "Number of unique lemmas: 4128\n",
      "Top 20 lemmas: [('“', 236), ('”', 217), ('’', 108), ('say', 51), ('one', 39), ('man', 39), ('upon', 38), ('holmes', 37), ('well', 31), ('see', 31), ('‘', 31), ('know', 28), ('hat', 25), ('goose', 25), ('little', 24), ('bird', 22), ('would', 22), ('go', 21), ('stone', 20), ('baker', 18)]\n",
      "------------------------------------------------------------\n",
      "Title: VIII. THE ADVENTURE OF THE SPECKLED BAND\n",
      "Number of tokens: 5056\n",
      "Number of unique words: 2028\n",
      "Number of unique lemmas: 5056\n",
      "Top 20 lemmas: [('“', 247), ('”', 233), ('’', 67), ('holmes', 56), ('say', 54), ('upon', 41), ('one', 32), ('see', 32), ('room', 30), ('would', 28), ('shall', 25), ('come', 23), ('could', 23), ('bed', 22), ('think', 22), ('sister', 22), ('light', 21), ('roylott', 21), ('dr.', 20), ('must', 20)]\n",
      "------------------------------------------------------------\n",
      "Title: IX. THE ADVENTURE OF THE ENGINEER’S THUMB\n",
      "Number of tokens: 4207\n",
      "Number of unique words: 1750\n",
      "Number of unique lemmas: 4207\n",
      "Top 20 lemmas: [('“', 192), ('’', 126), ('”', 111), ('‘', 86), ('say', 56), ('come', 41), ('upon', 38), ('one', 35), ('time', 25), ('little', 25), ('go', 25), ('could', 24), ('would', 23), ('see', 22), ('colonel', 21), ('door', 20), ('think', 20), ('u', 19), ('hand', 19), ('shall', 19)]\n",
      "------------------------------------------------------------\n",
      "Title: X. THE ADVENTURE OF THE NOBLE BACHELOR\n",
      "Number of tokens: 4256\n",
      "Number of unique words: 1746\n",
      "Number of unique lemmas: 4256\n",
      "Top 20 lemmas: [('“', 255), ('”', 247), ('’', 69), ('say', 49), ('st.', 42), ('simon', 38), ('lord', 37), ('holmes', 33), ('one', 30), ('make', 30), ('upon', 29), ('come', 28), ('think', 26), ('little', 25), ('see', 24), ('lady', 24), ('hand', 21), ('would', 20), ('take', 20), ('frank', 19)]\n",
      "------------------------------------------------------------\n",
      "Title: XI. THE ADVENTURE OF THE BERYL CORONET\n",
      "Number of tokens: 4704\n",
      "Number of unique words: 1785\n",
      "Number of unique lemmas: 4704\n",
      "Top 20 lemmas: [('“', 234), ('”', 163), ('’', 97), ('‘', 66), ('say', 60), ('could', 36), ('go', 35), ('upon', 33), ('think', 33), ('would', 32), ('one', 31), ('come', 29), ('see', 29), ('holmes', 27), ('man', 27), ('may', 25), ('coronet', 25), ('know', 24), ('tell', 23), ('hand', 22)]\n",
      "------------------------------------------------------------\n",
      "Title: XII. THE ADVENTURE OF THE COPPER BEECHES\n",
      "Number of tokens: 5033\n",
      "Number of unique words: 1889\n",
      "Number of unique lemmas: 5033\n",
      "Top 20 lemmas: [('“', 230), ('”', 143), ('’', 131), ('‘', 76), ('say', 51), ('holmes', 43), ('mr.', 43), ('little', 37), ('rucastle', 37), ('miss', 36), ('could', 36), ('would', 36), ('man', 34), ('one', 34), ('upon', 33), ('come', 32), ('door', 29), ('look', 28), ('go', 28), ('think', 26)]\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE STARTS HERE\n",
    "for key, value in get_book_dict(raw_text).items():\n",
    "    tokens, lemmas = preprocess(value)\n",
    "    joint_token = []\n",
    "    num_tokens = 0\n",
    "    num_types = 0\n",
    "\n",
    "    for token in tokens:\n",
    "        joint_token += token\n",
    "\n",
    "    freqs = Counter(lemmas)\n",
    "\n",
    "    print('Title:', key.rstrip('\\r'))\n",
    "    print('Number of tokens:', len(joint_token))\n",
    "    print('Number of unique words:', len(set(joint_token)))\n",
    "    print('Number of unique lemmas:', len(lemmas))\n",
    "    print('Top 20 lemmas:', freqs.most_common(20))\n",
    "    print('---'*20)\n",
    "# YOUR CODE ENDS HERE\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
