{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab 1. NLP Basics\n",
    "### Text preprocessing\n",
    "Text preprocessing is, probably, one of the least pleasant yet one of the most important steps of a natural language processing (NLP) pipelines. This step determines how your NLP algorithms are going to see the data. If your preprocessing breaks, the whole model can break or, what is even worse, keep silent and give incorrect results.\n",
    "\n",
    "Text preprocessing can be devided into three main parts:\n",
    "- Tokenization\n",
    "- Normalization\n",
    "- Noise reduction\n",
    "\n",
    "The parts are not necessarily applied in that particular order. Sometimes, before tokenization the noise reduction should be performed. In other cases, the some steps can be repeated several times.\n",
    "\n",
    "In the next steps, we are going to look into more details for each part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "from nltk import word_tokenize, sent_tokenize, pos_tag, pos_tag_sents\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.corpus import stopwords, wordnet, gutenberg, inaugural\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to install all the necessary files for NLTK\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('gutenberg')\n",
    "nltk.download('inaugural')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "__Tokenization__ is a general term for splitting the text into smaller parts. We can highlight __word segmentation__ and __sentence segmentation__. Depending on the task, you might need to use only word segmentation, for other tasks, you might want to have both sentences and words.\n",
    "\n",
    "As the names suggest, _word segmentation_ is dividing the raw text sequence into words and _sentence segmentation_ is dividing the text into sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagine that we need to parse the first paragraph from the Wikipedia article about Hawaii. We have the following raw text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_text = \"Hawaii is a state of the United States of America. \" + \\\n",
    "           \"It is the only state located in the Pacific Ocean and the only state composed entirely of islands.\"\n",
    "print(raw_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simplest and the most logical way to split the text into tokens would be to split it by the whitespace:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = raw_text.split()\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But already here, we can see the problem with the tokens like `'America.'` and `'islands.'`. In our case, the dot is the part of the token that we definetely don't want. One solution is to strip each token from the punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    return [token.strip(punctuation) for token in text.split()]\n",
    "\n",
    "print(tokenize(raw_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say now, that we want to split the text into sentences and then get tokens for each sentence. The simplest way is to split the text by dot first and then get tokens for each sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment_sents(text):\n",
    "    sents = []    # We are going to store the tokenized sentences here\n",
    "    for sent in text.split('.'):\n",
    "        # First check if the sentence is not empty\n",
    "        if sent: \n",
    "            # Then we do the same process for each token in the sentence\n",
    "            sents.append([token.strip(punctuation) for token in sent.split()])\n",
    "    return sents\n",
    "\n",
    "print(segment_sents(raw_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this example, it worked fine so far. But this task hold many surprises for an unprepared person. Let's see another examples that can cause troubles if using our function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "difficult_sents = [\n",
    "    \"Dr. Ford did not ask Col. Mustard the name of Mr. Smith's dog.\",\n",
    "    '\"What is all the fuss about?\" asked Mr. Peters.',\n",
    "    \"This full-time student isn't living in on-campus housing, and she's not wanting to visit Hawai'i.\"\n",
    "]\n",
    "\n",
    "for sent in difficult_sents:\n",
    "    print(segment_sents(sent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we can see that different abbreviations like *Dr.*, *Col.*, *Mr.* were treated as a sentence end. Also, contractions like _isn't_ and _she's_ are in fact two words: _is not_ and _she is_. However, _Smith's_ can be either _Smith is_ or rather, like in our case, one word showing possession. Finally, we have to decide if _full-time_ and _on-campus_ have one word or two.\n",
    "\n",
    "Luckily, for English, we can use different libraries like __nltk__ or __spacy__ which tackle most of these problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"NLTK tokenization:\\n\")\n",
    "for sent in difficult_sents:\n",
    "    print([word_tokenize(s) for s in sent_tokenize(sent)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization\n",
    "\n",
    "Normalization is another important step in text preprocessing since it removes a lot of input information and makes it easier for the model to choose the most important things. Two main steps in normalization are __stemming__ and __lemmatization__. \n",
    "_Stemming_ usually refers to removing endings and prefixes from a word. For example, `playing` and `played` are going to be reduced to `play` after going through the stemmer. It works rather well for English but it can be troublesome for other languages with not so straightforward morphology. Also, the past tense for `run`,  `ran` is not going to be changed with stemming and finally is going to be considered a different word.\n",
    "\n",
    "NLTK library includes a stemming package as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_to_stem = ['playing', 'played', 'plays', 'play', 'running', 'ran', 'runs', 'run']\n",
    "stemmer = PorterStemmer()\n",
    "print('Stemming with NLTK:\\n')\n",
    "for word in words_to_stem:\n",
    "    print(f'{word}: {stemmer.stem(word)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To solve the problem with the words that change their roots in different grammarical forms, we should use more complicated method, called _lemmatization_. Lemmatization usually uses more sophisticated rules to find the normal form of the word. Now, however, most of the lemmatizers are trained using neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Lemmatization with NLTK:\\n')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "for word in words_to_stem:\n",
    "    print(f'{word}: {lemmatizer.lemmatize(word)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see immediately that NLTK doesn't give correct lemmas for our words. This is because the NLTK lemmarizer expects to have a part-of-speech (POS) tag for each word, i.e. the information if the word is a noun, a verb, an adjective etc. We can, of course, specify the pos tag for each word but if our corpus is big, it will be tiresome to determine the pos tags by hand. In order to do that, we can use already pretrained pos tagger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Lemmatization with NLTK with correct pos tags:\\n')\n",
    "for word in words_to_stem:\n",
    "    print(f'{word}: {lemmatizer.lemmatize(word, pos=wordnet.VERB)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use [`pos_tag()`](https://www.nltk.org/api/nltk.tag.html?highlight=pos_tag#nltk.tag.pos_tag) function from the NLTK to automatically assign pos tags to each word in a sentence. This function uses NLTK's currently recommended part of speech tagger, which is the [Perceptron Tagger](https://explosion.ai/blog/part-of-speech-pos-tagger-in-python) now, to tag the given list of tokens. It offers a decent performance at a high speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_tag(word_tokenize(difficult_sents[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "360171427411ffad0ea618e36d69c617",
     "grade": false,
     "grade_id": "cell-4cf67a3fb1e4dee8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "However, the default NLTK tagger uses [the Penn Treebank part-of-speech tags](https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html) while the lemmatizer takes WordNet tags. WordNet tags are the following: `'n'` for nouns, `'v'` for verbs, `'a'` for adjectives, `'r'` for adverbs. To make then work together, we will need to write a small function to convert one tagset to another. \n",
    "\n",
    "It can be done either with or withour regular expressions. Try both ways and compare!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f06cfabecb01d6b2a1f9bedd3e5f1467",
     "grade": false,
     "grade_id": "cell-2424b31c691972df",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def treebank_to_wordnet(tag):\n",
    "    \"\"\"Converts Penn Treebank part-of-speech tag into WordNet pos tag.\n",
    "    \n",
    "    Args:\n",
    "        tag (str): Penn Treebank part-of-speech tag.\n",
    "        \n",
    "    Returns:\n",
    "        str: WordNet part-of-speech tag. If the input tag is unknown, return the noun pos tag.\n",
    "        \n",
    "    \"\"\"\n",
    "    # YOUR CODE STARTS HERE\n",
    "    # YOUR CODE ENDS HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "381db1a0bcb35a369c7705175060ed46",
     "grade": true,
     "grade_id": "cell-bbfdba44c9cb7cab",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert treebank_to_wordnet('NN') == wordnet.NOUN\n",
    "assert treebank_to_wordnet('NNPS') == wordnet.NOUN\n",
    "\n",
    "assert treebank_to_wordnet('VB') == wordnet.VERB\n",
    "assert treebank_to_wordnet('VBG') == wordnet.VERB\n",
    "\n",
    "assert treebank_to_wordnet('JJ') == wordnet.ADJ\n",
    "assert treebank_to_wordnet('JJS') == wordnet.ADJ\n",
    "\n",
    "assert treebank_to_wordnet('RB') == wordnet.ADV\n",
    "assert treebank_to_wordnet('RBR') == wordnet.ADV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0795f551c2ba8d8abf5adfc83bd18ed0",
     "grade": false,
     "grade_id": "cell-1a365d22b4aafde8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Now we can process our sentences from before and see how they look lemmatized. Also, here we will use a `pos_tag_sents()` function, which takes a list of lists of tokens, i.e. it can work with multiple sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8f0e6325dc12f34d9e64b1f4cccd0732",
     "grade": false,
     "grade_id": "cell-51b078fa8655599c",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def nltk_lemmatize(sents):\n",
    "    \"\"\"Takes in the tokenized sentences and converts each token into the lemma.\n",
    "    \n",
    "    Args:\n",
    "        sents (list[list[str]]): List of tokenized sentences.\n",
    "        \n",
    "    Returns:\n",
    "        list[list[str]]: List of lemmatized sentences.\n",
    "        \n",
    "    \"\"\"\n",
    "    nltk_lemmas = []\n",
    "    \n",
    "    # YOUR CODE STARTS HERE\n",
    "    # YOUR CODE ENDS HERE\n",
    "    \n",
    "    return nltk_lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "be1320df038df5b4bca2bca3e7e86b7d",
     "grade": true,
     "grade_id": "cell-94559e4d25f1bfb4",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "sent_raw = [word_tokenize(s) for s in sent_tokenize(difficult_sents[0])]\n",
    "sent_lem = [['Dr.', 'Ford', 'do', 'not', 'ask', 'Col.', 'Mustard', 'the', 'name', 'of', 'Mr.', 'Smith', \"'s\", 'dog', '.']]\n",
    "assert nltk_lemmatize(sent_raw)  == sent_lem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sent in difficult_sents:\n",
    "    nltk_sents = [word_tokenize(s) for s in sent_tokenize(sent)]\n",
    "    print(f'Original sentence:\\n{nltk_sents}')\n",
    "    print(f'Tagged sentence:\\n{pos_tag_sents(nltk_sents)}')    \n",
    "    print(f'Lemmatized sentence:\\n{nltk_lemmatize(nltk_sents)}')\n",
    "    print('\\n------\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also compare it to the stemming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "78be82a6d45a57ffd2aafcb3063e5508",
     "grade": false,
     "grade_id": "cell-5d6603a966154ebd",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def nltk_stem(sents):\n",
    "    \"\"\"Takes in the tokenized sentences and converts each token into the stem.\n",
    "    \n",
    "    Args:\n",
    "        sents (list[list[str]]): List of tokenized sentences.\n",
    "        \n",
    "    Returns:\n",
    "        list[list[str]]: List of stemmed sentences.\n",
    "    \n",
    "    \"\"\"    \n",
    "    nltk_stems = []\n",
    "    \n",
    "    # YOUR CODE STARTS HERE\n",
    "    # YOUR CODE ENDS HERE\n",
    "    \n",
    "    return nltk_stems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fca1f1b10e2ae088b0f8e04049b3c79b",
     "grade": true,
     "grade_id": "cell-3a63e0a4635e28ad",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "sent_raw = [word_tokenize(s) for s in sent_tokenize(difficult_sents[0])]\n",
    "sent_stem = [['dr.', 'ford', 'did', 'not', 'ask', 'col.', 'mustard', 'the', 'name', 'of', 'mr.', 'smith', \"'s\", 'dog', '.']]\n",
    "assert nltk_stem(sent_raw) == sent_stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"NLTK stemming:\\n\")\n",
    "for sent in difficult_sents:\n",
    "    nltk_sents = [word_tokenize(s) for s in sent_tokenize(sent)]\n",
    "    print(f'Original sentence:\\n{nltk_sents}')\n",
    "    print(f'Stemmed sentence:\\n{nltk_stem(nltk_sents)}')\n",
    "    print('\\n------\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3e1b7eae33a33ce78c19d53259561050",
     "grade": false,
     "grade_id": "cell-6e5e49b36be5396d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "We can see the NLTK stemmer also __puts all the words to lowercase__ which is another part of normalization. Also, we can also see some artifacts with the stemming like `thi`, `full-tim`, `on-campu`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c9051c7aab028e368f273fb218f82d98",
     "grade": false,
     "grade_id": "cell-5ae852b101c38e19",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "We can now try to work with a slightly bigger text. NLTK has different [corpora](https://www.nltk.org/nltk_data/) available which are ready for you to use. You can see a bit more on how to use them here: https://www.nltk.org/howto/corpus.html.\n",
    "\n",
    "For example, we can use the corpus of inaugural speeches of US presidents. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inaugural.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_fileid = '1789-Washington.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(inaugural.raw(text_fileid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can make a simple overview of the text by counting the number of words and vocabulary size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = inaugural.words(text_fileid)\n",
    "print(len(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = set(words)\n",
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We probably also do not really care about the punctuation here, so we can filter it out as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_lower = [word.lower() for word in words if word not in punctuation]\n",
    "vocab_lower = set(words_lower)\n",
    "print(len(vocab_lower))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can see that the vocabulary size is almost three times smaller than the number of the words in the text. \n",
    "\n",
    "Interestingly, the relation between the number of words and vocabulary size is decribed by [Heaps' law](https://en.wikipedia.org/wiki/Heaps%27_law). In general, the bigger is the text, the slower is the vocabulary growth. Try to use a bigger text to check it!\n",
    "\n",
    "One more way to analyze the text is to make a frequency list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freqs = Counter(words_lower)\n",
    "freqs.most_common(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not very informative, right? That's because very common words like articles (a, the) and prepositions (to, in, by) are overwhelming the other words. These are usually called __stop words__ and can be filtered out as a part of __text normalization__ process.\n",
    "\n",
    "P.S. Look how the word frequencies roughly follow [Zipf's law](https://en.wikipedia.org/wiki/Zipf%27s_law)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another parts for the normalization include:\n",
    "- Removing the punctuation\n",
    "- Removing whitespace\n",
    "- Removing numbers or converting them into text\n",
    "- Converting contractions to their full forms (I've -> I have)\n",
    "\n",
    "Finally, we can look a bit more into the __stop words__. Stop words are the words that are very common in some language but usually don't carry any useful information about the idea of the text. For English, they can be _is_, _are_, _not_, _she_, _he_, _it_ etc. This also usually includes prepositions and other particles. However, the stop list can be modified to fit a specific task.\n",
    "\n",
    "Both NLTK and Spacy have built-in lists for stop words, however, you are free to find it anywhere else on the internet or even compose your own list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Stop words for English from NLTK:\\n')\n",
    "nltk_stopwords = set(stopwords.words('English'))\n",
    "print(sorted(nltk_stopwords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_stop_words = [word for word in words_lower if word not in nltk_stopwords.union(punctuation)]\n",
    "freqs = Counter(no_stop_words)\n",
    "freqs.most_common(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A bit better already! Now, if we show this wordlist to someone who doesn't know which text it was taken from, they would probably guess that it has to do something with politics. \n",
    "\n",
    "We can also plot it :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fd = nltk.FreqDist(no_stop_words)\n",
    "plt.figure(figsize=(10, 5))\n",
    "fd.plot(30);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also see the trends in the US presidential speeches. For example, we can plot how the words \"citizen\" and \"america\" were used over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 5))\n",
    "cfd = nltk.ConditionalFreqDist(\n",
    "           (target, fileid[:4])\n",
    "           for fileid in inaugural.fileids()\n",
    "           for w in inaugural.words(fileid)\n",
    "           for target in ['america', 'citizen']\n",
    "           if w.lower().startswith(target))\n",
    "cfd.plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last but not least, you can create a cloud of words to represent any text. You have probably seen it before, since it appears rather frequently in NLP blogs. Luckily, it is very easy to create it using the [wordcloud](https://pypi.org/project/wordcloud/) package!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud = WordCloud().generate_from_frequencies(freqs)\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Noise Removal\n",
    "\n",
    "In this lab, we are not going to go into details of this step. It includes:\n",
    "- Removal of headers, footers and other parts of the articles\n",
    "- Removal of HTML, XML etc. markup\n",
    "- Extracting the data from various formats, like JSON, CONLL etc.\n",
    "\n",
    "Most of these steps can be done with the regular expressions. There are also good libraries out there to help you. For example, [Beautiful Soup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/) is a very powerful tool for the HTML and XML parsing."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
