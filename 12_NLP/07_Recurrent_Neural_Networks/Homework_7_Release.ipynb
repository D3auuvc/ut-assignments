{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JJ-jBGvGtyBd"
   },
   "source": [
    "## Homework 7\n",
    "### Recurrent Neural Networks\n",
    "\n",
    "Welcome to Homework 7! \n",
    "\n",
    "The homework contains several tasks. You can find the amount of points that you get for the correct solution in the task header. Maximum amount of points for each homework is _four_.\n",
    "\n",
    "The **grading** for each task is the following:\n",
    "- correct answer - **full points**\n",
    "- insufficient solution or solution resulting in the incorrect output - **half points**\n",
    "- no answer or completely wrong solution - **no points**\n",
    "\n",
    "Even if you don't know how to solve the task, we encourage you to write down your thoughts and progress and try to address the issues that stop you from completing the task.\n",
    "\n",
    "When working on the written tasks, try to make your answers short and accurate. Most of the times, it is possible to answer the question in 1-3 sentences.\n",
    "\n",
    "When writing code, make it readable. Choose appropriate names for your variables (`a = 'cat'` - not good, `word = 'cat'` - good). Avoid constructing lines of code longer than 100 characters (79 characters is ideal). If needed, provide the commentaries for your code, however, a good code should be easily readable without them :)\n",
    "\n",
    "Finally, all your answers should be written only by yourself. If you copy them from other sources it will be considered as an academic fraud. You can discuss the tasks with your classmates but each solution must be individual.\n",
    "\n",
    "<font color='red'>**Important!:**</font> **before sending your solution, do the `Kernel -> Restart & Run All` to ensure that all your code works.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "af3Q_oLQ0SoU"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting conllu\n",
      "  Using cached conllu-4.4.1-py2.py3-none-any.whl (15 kB)\n",
      "Installing collected packages: conllu\n",
      "\u001b[31mERROR: Could not install packages due to an OSError: [Errno 13] Permission denied: '/gpfs/space/software/jupyterhub/python/jupyter/lib/python3.10/site-packages/conllu'\n",
      "Check the permissions.\n",
      "\u001b[0m\n",
      "\u001b[33mWARNING: You are using pip version 21.3.1; however, version 22.0.4 is available.\n",
      "You should consider upgrading via the '/gpfs/space/software/jupyterhub/python/jupyter/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install conllu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "yxsccUZF0cKl"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-04-05 12:38:08--  https://lindat.mff.cuni.cz/repository/xmlui/bitstream/handle/11234/1-4611/ud-treebanks-v2.9.tgz\n",
      "Resolving lindat.mff.cuni.cz (lindat.mff.cuni.cz)... 195.113.20.140\n",
      "Connecting to lindat.mff.cuni.cz (lindat.mff.cuni.cz)|195.113.20.140|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 463076751 (442M) [application/x-gzip]\n",
      "Saving to: ‘ud-treebanks-v2.9.tgz’\n",
      "\n",
      "100%[======================================>] 463,076,751 25.2MB/s   in 11s    \n",
      "\n",
      "2022-04-05 12:38:19 (41.2 MB/s) - ‘ud-treebanks-v2.9.tgz’ saved [463076751/463076751]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://lindat.mff.cuni.cz/repository/xmlui/bitstream/handle/11234/1-4611/ud-treebanks-v2.9.tgz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "G7AdR5Du0vwK"
   },
   "outputs": [],
   "source": [
    "!tar -xzf ud-treebanks-v2.9.tgz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "376OmYsZWlu2"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TORCH_SHOW_CPP_STACKTRACES\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "RRMN0tVS0Y1z"
   },
   "outputs": [],
   "source": [
    "from conllu import parse, TokenList\n",
    "from pathlib import Path\n",
    "from torchtext.vocab import build_vocab_from_iterator, Vocab\n",
    "from collections import OrderedDict, defaultdict\n",
    "import typing\n",
    "from typing import List, Dict\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BUznucJ6tyBl"
   },
   "source": [
    "## Task 0. Load the data for your language\n",
    "\n",
    "Choose the correct folder and short treebank code for your language. For example, for Estonian EDT treebank, the `treebank_dir` will be `ud-treebanks-v2.9/UD_Estonian-EDT/` and `treebank_short` will be `et_edt`.\n",
    "\n",
    "Ideally, you should use the same dataset that you used in the practice session. If you missed it, just find a dataset for your native language. If your native language is not represented in the Universal Dependencies, you can use any other language that you know (e.g. English)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "id": "h9OdbH0b1F6s"
   },
   "outputs": [],
   "source": [
    "treebank_dir = Path('ud-treebanks-v2.9/UD_Japanese-GSD/')\n",
    "treebank_short = 'ja_gsd'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "id": "yGh13jfP1dmP"
   },
   "outputs": [],
   "source": [
    "train_data = parse(open(treebank_dir / f'{treebank_short}-ud-train.conllu', encoding='utf-8').read())\n",
    "valid_data = parse(open(treebank_dir / f'{treebank_short}-ud-dev.conllu', encoding='utf-8').read())\n",
    "test_data = parse(open(treebank_dir / f'{treebank_short}-ud-test.conllu', encoding='utf-8').read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "id": "L-eUOYm51xVu"
   },
   "outputs": [],
   "source": [
    "word_vocab = build_vocab_from_iterator(\n",
    "    ([token[\"form\"] for token in sent] for sent in train_data), \n",
    "    specials=[\"<pad>\", \"<unk>\"]\n",
    ")\n",
    "word_vocab.set_default_index(word_vocab[\"<unk>\"])\n",
    "\n",
    "upos_vocab = build_vocab_from_iterator(\n",
    "    ([token[\"upos\"] for token in sent] for sent in train_data), \n",
    "    specials=[\"<pad>\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "HaujLlRAgjAt"
   },
   "outputs": [],
   "source": [
    "feats_vocab = build_vocab_from_iterator(\n",
    "    ([\"|\".join([k + '=' + v for k, v in token[\"feats\"].items()]) if token[\"feats\"] else \"_\" for token in sent] \n",
    "     for sent in train_data),\n",
    "     specials=[\"<pad>\"])\n",
    "feats_vocab.set_default_index(feats_vocab[\"_\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TokenList<統一, 教会, に, 導ける, !>\n"
     ]
    }
   ],
   "source": [
    "print(train_data[10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EvCjPpVwtyBq"
   },
   "source": [
    "printask 1. Create a character vocab (0.25 points)\n",
    "\n",
    "Using the training data, create a vocabulary of each character in the training data. I should be similar to the `word_vocab`, but instead of tokens you would have characters. Add `\"<pad>\"` and `\"<unk>\"` special tokens. Make index for `\"<unk>\"` token default for the vocab.\n",
    "\n",
    "The `stoi` dict should look similar to this (it will not be exactly the same, just make sure that each key is a single character):\n",
    "\n",
    "```\n",
    "{'Ð': 106, 'g': 16, '”': 71, '<pad>': 0, '<unk>': 1, ')': 43, 'i': 4, '!': 61, 'Ö': 92, 'a': 2, 'e': 3, '3': 52, 'Ä': 82, 's': 5, ',': 23, 'l': 7, '4': 60, 't': 6, '5': 51, 'u': 8, 'ū': 127, '.': 20, 'n': 9, 'Ü': 64, 'k': 10, 'd': 11, 'o': 12, '-': 30, 'm': 13, '2': 44, 'r': 14, '6': 58, 'v': 15, '0': 35, 'Ç': 121, 'p': 17, '(': 45, 'h': 18, 'j': 19, 'ä': 21, 'S': 28, 'õ': 22, 'B': 59, '\"': 29, 'Õ': 80, 'b': 24, 'ü': 25, 'K': 27, 'ö': 26, 'A': 37, 'T': 31, 'E': 32, 'M': 33, '1': 34, 'ç': 104, 'P': 36, 'f': 38, '9': 39, 'á': 118, 'V': 40, 'L': 41, 'N': 42, 'þ': 99, 'I': 46, 'å': 122, 'R': 47, 'J': 48, 'H': 49, 'ø': 119, 'O': 50, ':': 53, '?': 54, 'U': 55, 'Ž': 113, '8': 56, 'c': 57, '7': 62, 'ó': 110, 'D': 63, 'ð': 107, 'G': 65, 'C': 66, 'y': 67, '$': 114, 'š': 68, 'Š': 91, '%': 69, '“': 70, 'ñ': 125, 'F': 72, 'z': 73, 'w': 74, 'ž': 84, ';': 75, \"'\": 76, 'x': 77, '/': 78, 'ω': 79, 'à': 117, 'W': 81, 'X': 83, 'Y': 85, 'í': 124, 'Z': 86, '[': 87, ']': 88, '·': 89, '…': 90, 'q': 93, 'æ': 111, 'Q': 94, '&': 95, 'é': 96, '–': 97, '>': 98, '@': 100, '=': 101, '+': 102, '²': 103, '_': 105, '*': 108, '§': 109, '•': 112, '<': 115, '°': 116, '~': 120, 'ë': 123, 'ē': 126}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "id": "s6akibcztyBr"
   },
   "outputs": [],
   "source": [
    "# TODO: Build a character-level vocab\n",
    "char_vocab = build_vocab_from_iterator(\n",
    "    ( [ char for char in words] for words in [tokens.metadata['text'] for tokens in train_data]), \n",
    "    specials=[\"<pad>\", \"<unk>\"])\n",
    "# TODO: Set the default index\n",
    "char_vocab.set_default_index(char_vocab[\"<unk>\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yt_ZdSKetyBs"
   },
   "source": [
    "## Task 2. Add character representation of words (0.25 points)\n",
    "\n",
    "Modify the dataset to return the character indices of each token. For example, for the token `cat` and the vocab `{'<pad>': 0, '<unk>': 1, 'a': 2, 'c': 3, 't': 4}` you should return `[3, 2, 4]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {
    "id": "kXY7VNoH7PAJ"
   },
   "outputs": [],
   "source": [
    "class UDDataset(Dataset):\n",
    "    def __init__(self, data: List[TokenList], \n",
    "                 vocabs: Dict[str, Vocab], \n",
    "                 device: torch.device):\n",
    "        super().__init__()\n",
    "        self.data = data\n",
    "        self.vocabs = vocabs\n",
    "        self.device = device\n",
    "        self.preprocess()\n",
    "\n",
    "    def preprocess(self):\n",
    "        self.preprocessed = []\n",
    "        for sent in self.data:\n",
    "            # TODO: Return character indices for each token\n",
    "            chars=[]\n",
    "            tokens = [ token[\"form\"] for token in sent ] \n",
    "            for token in tokens:\n",
    "                lst=[]\n",
    "                for t in list(token):\n",
    "                    try:\n",
    "                        lst.append(self.vocabs[\"char\"].get_stoi()[t])\n",
    "                    except KeyError:\n",
    "                        lst.append(self.vocabs[\"char\"].get_default_index())\n",
    "                chars.append(lst)\n",
    "            \n",
    "            words = self.vocabs[\"word\"]([token[\"form\"] for token in sent])\n",
    "            upos = self.vocabs[\"upos\"]([token[\"upos\"] for token in sent])\n",
    "            feats_raw = [\"|\".join([k + '=' + v for k, v in token[\"feats\"].items()]) \n",
    "                         if token[\"feats\"] else \"_\" for token in sent]\n",
    "            feats = self.vocabs[\"feats\"](feats_raw)\n",
    "\n",
    "            chars = [torch.tensor(char, dtype=torch.long, device=self.device) for char in chars]\n",
    "            words = torch.tensor(words, dtype=torch.long, device=self.device)\n",
    "            upos = torch.tensor(upos, dtype=torch.long, device=self.device)\n",
    "            feats = torch.tensor(feats, dtype=torch.long, device=self.device)\n",
    "\n",
    "            self.preprocessed.append((chars, words, upos, feats))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.preprocessed[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.preprocessed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {
    "id": "yNWMfeOJ7gvD"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current device is: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "# device = torch.device(\"cpu\")\n",
    "print(\"Current device is:\", device)\n",
    "\n",
    "vocabs = {\"char\": char_vocab, \"word\": word_vocab, \"upos\": upos_vocab, \"feats\": feats_vocab}\n",
    "train_dataset = UDDataset(train_data, vocabs, device)\n",
    "dev_dataset = UDDataset(valid_data, vocabs, device)\n",
    "test_dataset = UDDataset(test_data, vocabs, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "41uOq0QXtyBu"
   },
   "source": [
    "You can use these two cells to test if your output makes sense. Look through each character encoded token and compare it with the original token. The number of characters (letters) must be the same for each token.\n",
    "\n",
    "For example:\n",
    "\n",
    "```\n",
    "([tensor([46, 16,  2], device='cuda:0'),\n",
    "  tensor([25, 18,  3, 10,  5,  2,  5], device='cuda:0'),\n",
    "  tensor([10, 14, 12, 12,  9], device='cuda:0'),\n",
    "  tensor([6, 8, 7, 4], device='cuda:0'),\n",
    "  tensor([ 5,  2,  7,  2, 17, 21, 14,  2,  5,  6,  3,  7,  6], device='cuda:0'),\n",
    "  tensor([ 4,  5,  4, 10,  8,  6,  3,  7,  6], device='cuda:0'),\n",
    "  tensor([20], device='cuda:0')],\n",
    " tensor([  646, 15186, 12842,   172, 62115, 44158,     2], device='cuda:0'),\n",
    " tensor([13,  5,  1,  3,  5,  1,  2], device='cuda:0'),\n",
    " tensor([ 94, 119,   2,   6, 313, 154,   1], device='cuda:0'))\n",
    "```\n",
    "\n",
    "```\n",
    "# newdoc id = aja_ee199920\n",
    "# sent_id = aja_ee199920_1\n",
    "# text = Iga üheksas kroon tuli salapärastelt isikutelt.\n",
    "1\tIga\tiga\tDET\tP\tCase=Nom|Number=Sing|PronType=Tot\t3\tdet\t3:det\t_\n",
    "2\tüheksas\tüheksas\tADJ\tN\tCase=Nom|Number=Sing|NumForm=Word|NumType=Ord\t3\tamod\t3:amod\t_\n",
    "3\tkroon\tkroon\tNOUN\tS\tCase=Nom|Number=Sing\t4\tnsubj\t4:nsubj\t_\n",
    "4\ttuli\ttulema\tVERB\tV\tMood=Ind|Number=Sing|Person=3|Tense=Past|VerbForm=Fin|Voice=Act\t0\troot\t0:root\t_\n",
    "5\tsalapärastelt\tsala_pärane\tADJ\tA\tCase=Abl|Degree=Pos|Number=Plur\t6\tamod\t6:amod\t_\n",
    "6\tisikutelt\tisik\tNOUN\tS\tCase=Abl|Number=Plur\t4\tobl\t4:obl\tSpaceAfter=No\n",
    "7\t.\t.\tPUNCT\tZ\t_\t4\tpunct\t4:punct\t_\n",
    "```\n",
    "\n",
    "See how the token `Iga` is encoded as `tensor([46, 16,  2], device='cuda:0')`, `üheksas` as `tensor([25, 18,  3, 10,  5,  2,  5], device='cuda:0')` and so on. Your results should follow a similar pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {
    "id": "ct7iQs2rBGjw"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([tensor([255,  46, 248,  16], device='cuda:0'),\n",
       "  tensor([4], device='cuda:0'),\n",
       "  tensor([11], device='cuda:0'),\n",
       "  tensor([ 97,  19,  76, 130,  41,  29], device='cuda:0'),\n",
       "  tensor([86, 67, 16], device='cuda:0'),\n",
       "  tensor([2], device='cuda:0'),\n",
       "  tensor([ 418, 1226], device='cuda:0'),\n",
       "  tensor([15], device='cuda:0'),\n",
       "  tensor([31,  8], device='cuda:0'),\n",
       "  tensor([2], device='cuda:0'),\n",
       "  tensor([10], device='cuda:0'),\n",
       "  tensor([5], device='cuda:0'),\n",
       "  tensor([2297], device='cuda:0'),\n",
       "  tensor([45, 26], device='cuda:0'),\n",
       "  tensor([95], device='cuda:0'),\n",
       "  tensor([4], device='cuda:0'),\n",
       "  tensor([259,  16,  32], device='cuda:0'),\n",
       "  tensor([14], device='cuda:0'),\n",
       "  tensor([1204,   22,   20], device='cuda:0'),\n",
       "  tensor([27, 12], device='cuda:0'),\n",
       "  tensor([11], device='cuda:0'),\n",
       "  tensor([512,  80], device='cuda:0'),\n",
       "  tensor([109], device='cuda:0'),\n",
       "  tensor([4], device='cuda:0'),\n",
       "  tensor([ 418, 1226], device='cuda:0'),\n",
       "  tensor([4], device='cuda:0'),\n",
       "  tensor([17,  8], device='cuda:0'),\n",
       "  tensor([15], device='cuda:0'),\n",
       "  tensor([5], device='cuda:0'),\n",
       "  tensor([53,  2], device='cuda:0'),\n",
       "  tensor([678, 265], device='cuda:0'),\n",
       "  tensor([2], device='cuda:0'),\n",
       "  tensor([68], device='cuda:0'),\n",
       "  tensor([66], device='cuda:0'),\n",
       "  tensor([15], device='cuda:0'),\n",
       "  tensor([27,  2], device='cuda:0'),\n",
       "  tensor([29, 48, 16, 86], device='cuda:0'),\n",
       "  tensor([10], device='cuda:0'),\n",
       "  tensor([31,  8], device='cuda:0'),\n",
       "  tensor([3], device='cuda:0')],\n",
       " tensor([12579,     4,     6, 11771,  1917,     2,  6988,    10,    20,     2,\n",
       "            11,     5,  8240,    70,   124,     4,  1157,     7, 17198,    26,\n",
       "             6,   610,    40,     4,  6988,     4,    63,    10,     5,    37,\n",
       "          3016,     2,    78,    92,    10,    41, 11368,    11,    20,     3],\n",
       "        device='cuda:0'),\n",
       " tensor([ 1,  2,  2,  1,  1,  2,  1,  2,  3,  6,  4,  5,  1,  2,  1,  2,  1,  2,\n",
       "          3,  1,  2,  1, 12,  4,  1,  2,  3,  6,  5, 14,  1,  2,  8,  1,  2, 14,\n",
       "          1,  4,  3,  5], device='cuda:0'),\n",
       " tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0'))"
      ]
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {
    "id": "ERqFe_iqBP7M"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# newdoc id = train-s1\n",
      "# sent_id = train-s1\n",
      "# text = ホッケーにはデンジャラスプレーの反則があるので、膝より上にボールを浮かすことは基本的に反則になるが、その例外の一つがこのスクープである。\n",
      "1\tホッケー\tホッケー\tNOUN\t名詞-普通名詞-一般\t_\t9\tobl\t_\tBunsetuBILabel=B|BunsetuPositionType=SEM_HEAD|LUWBILabel=B|LUWPOS=名詞-普通名詞-一般|SpaceAfter=No\n",
      "2\tに\tに\tADP\t助詞-格助詞\t_\t1\tcase\t_\tBunsetuBILabel=I|BunsetuPositionType=SYN_HEAD|LUWBILabel=B|LUWPOS=助詞-格助詞|SpaceAfter=No\n",
      "3\tは\tは\tADP\t助詞-係助詞\t_\t1\tcase\t_\tBunsetuBILabel=I|BunsetuPositionType=FUNC|LUWBILabel=B|LUWPOS=助詞-係助詞|SpaceAfter=No\n",
      "4\tデンジャラス\tデンジャラス\tNOUN\t名詞-普通名詞-一般\t_\t5\tcompound\t_\tBunsetuBILabel=B|BunsetuPositionType=CONT|LUWBILabel=B|LUWPOS=名詞-普通名詞-一般|SpaceAfter=No\n",
      "5\tプレー\tプレー\tNOUN\t名詞-普通名詞-サ変可能\t_\t7\tnmod\t_\tBunsetuBILabel=I|BunsetuPositionType=SEM_HEAD|LUWBILabel=I|LUWPOS=名詞-普通名詞-一般|SpaceAfter=No\n",
      "6\tの\tの\tADP\t助詞-格助詞\t_\t5\tcase\t_\tBunsetuBILabel=I|BunsetuPositionType=SYN_HEAD|LUWBILabel=B|LUWPOS=助詞-格助詞|SpaceAfter=No\n",
      "7\t反則\t反則\tNOUN\t名詞-普通名詞-サ変可能\t_\t9\tnsubj\t_\tBunsetuBILabel=B|BunsetuPositionType=SEM_HEAD|LUWBILabel=B|LUWPOS=名詞-普通名詞-一般|SpaceAfter=No\n",
      "8\tが\tが\tADP\t助詞-格助詞\t_\t7\tcase\t_\tBunsetuBILabel=I|BunsetuPositionType=SYN_HEAD|LUWBILabel=B|LUWPOS=助詞-格助詞|SpaceAfter=No\n",
      "9\tある\tある\tVERB\t動詞-非自立可能-五段-ラ行\t_\t19\tadvcl\t_\tBunsetuBILabel=B|BunsetuPositionType=SEM_HEAD|LUWBILabel=B|LUWPOS=動詞-一般-五段-ラ行|SpaceAfter=No\n",
      "10\tの\tの\tSCONJ\t助詞-準体助詞\t_\t9\tmark\t_\tBunsetuBILabel=I|BunsetuPositionType=SYN_HEAD|LUWBILabel=B|LUWPOS=助動詞-助動詞-ダ|SpaceAfter=No\n",
      "11\tで\tだ\tAUX\t助動詞-助動詞-ダ\t_\t10\tfixed\t_\tBunsetuBILabel=I|BunsetuPositionType=FUNC|LUWBILabel=I|LUWPOS=助動詞-助動詞-ダ|SpaceAfter=No\n",
      "12\t、\t、\tPUNCT\t補助記号-読点\t_\t9\tpunct\t_\tBunsetuBILabel=I|BunsetuPositionType=CONT|LUWBILabel=B|LUWPOS=補助記号-読点|SpaceAfter=No\n",
      "13\t膝\t膝\tNOUN\t名詞-普通名詞-一般\t_\t15\tnmod\t_\tBunsetuBILabel=B|BunsetuPositionType=SEM_HEAD|LUWBILabel=B|LUWPOS=名詞-普通名詞-一般|SpaceAfter=No\n",
      "14\tより\tより\tADP\t助詞-格助詞\t_\t13\tcase\t_\tBunsetuBILabel=I|BunsetuPositionType=SYN_HEAD|LUWBILabel=B|LUWPOS=助詞-格助詞|SpaceAfter=No\n",
      "15\t上\t上\tNOUN\t名詞-普通名詞-副詞可能\t_\t19\tobl\t_\tBunsetuBILabel=B|BunsetuPositionType=SEM_HEAD|LUWBILabel=B|LUWPOS=名詞-普通名詞-一般|SpaceAfter=No\n",
      "16\tに\tに\tADP\t助詞-格助詞\t_\t15\tcase\t_\tBunsetuBILabel=I|BunsetuPositionType=SYN_HEAD|LUWBILabel=B|LUWPOS=助詞-格助詞|SpaceAfter=No\n",
      "17\tボール\tボール\tNOUN\t名詞-普通名詞-一般\t_\t19\tobj\t_\tBunsetuBILabel=B|BunsetuPositionType=SEM_HEAD|LUWBILabel=B|LUWPOS=名詞-普通名詞-一般|SpaceAfter=No\n",
      "18\tを\tを\tADP\t助詞-格助詞\t_\t17\tcase\t_\tBunsetuBILabel=I|BunsetuPositionType=SYN_HEAD|LUWBILabel=B|LUWPOS=助詞-格助詞|SpaceAfter=No\n",
      "19\t浮かす\t浮かす\tVERB\t動詞-一般-五段-サ行\t_\t20\tacl\t_\tBunsetuBILabel=B|BunsetuPositionType=SEM_HEAD|LUWBILabel=B|LUWPOS=動詞-一般-五段-サ行|SpaceAfter=No\n",
      "20\tこと\tこと\tNOUN\t名詞-普通名詞-一般\t_\t27\tnsubj\t_\tBunsetuBILabel=B|BunsetuPositionType=SEM_HEAD|LUWBILabel=B|LUWPOS=名詞-普通名詞-一般|SpaceAfter=No\n",
      "21\tは\tは\tADP\t助詞-係助詞\t_\t20\tcase\t_\tBunsetuBILabel=I|BunsetuPositionType=SYN_HEAD|LUWBILabel=B|LUWPOS=助詞-係助詞|SpaceAfter=No\n",
      "22\t基本\t基本\tNOUN\t名詞-普通名詞-一般\t_\t27\tadvcl\t_\tBunsetuBILabel=B|BunsetuPositionType=SEM_HEAD|LUWBILabel=B|LUWPOS=形状詞-一般|SpaceAfter=No\n",
      "23\t的\t的\tPART\t接尾辞-形状詞的\t_\t22\tmark\t_\tBunsetuBILabel=I|BunsetuPositionType=SYN_HEAD|LUWBILabel=I|LUWPOS=形状詞-一般|SpaceAfter=No\n",
      "24\tに\tだ\tAUX\t助動詞-助動詞-ダ\t_\t22\tcop\t_\tBunsetuBILabel=I|BunsetuPositionType=FUNC|LUWBILabel=B|LUWPOS=助動詞-助動詞-ダ|SpaceAfter=No\n",
      "25\t反則\t反則\tNOUN\t名詞-普通名詞-サ変可能\t_\t27\tobl\t_\tBunsetuBILabel=B|BunsetuPositionType=SEM_HEAD|LUWBILabel=B|LUWPOS=名詞-普通名詞-一般|SpaceAfter=No\n",
      "26\tに\tに\tADP\t助詞-格助詞\t_\t25\tcase\t_\tBunsetuBILabel=I|BunsetuPositionType=SYN_HEAD|LUWBILabel=B|LUWPOS=助詞-格助詞|SpaceAfter=No\n",
      "27\tなる\tなる\tVERB\t動詞-非自立可能-五段-ラ行\t_\t37\tacl\t_\tBunsetuBILabel=B|BunsetuPositionType=SEM_HEAD|LUWBILabel=B|LUWPOS=動詞-一般-五段-ラ行|SpaceAfter=No\n",
      "28\tが\tが\tSCONJ\t助詞-接続助詞\t_\t27\tmark\t_\tBunsetuBILabel=I|BunsetuPositionType=SYN_HEAD|LUWBILabel=B|LUWPOS=助詞-接続助詞|SpaceAfter=No\n",
      "29\t、\t、\tPUNCT\t補助記号-読点\t_\t27\tpunct\t_\tBunsetuBILabel=I|BunsetuPositionType=CONT|LUWBILabel=B|LUWPOS=補助記号-読点|SpaceAfter=No\n",
      "30\tその\tその\tDET\t連体詞\t_\t31\tdet\t_\tBunsetuBILabel=B|BunsetuPositionType=SEM_HEAD|LUWBILabel=B|LUWPOS=連体詞|SpaceAfter=No\n",
      "31\t例外\t例外\tNOUN\t名詞-普通名詞-一般\t_\t34\tnmod\t_\tBunsetuBILabel=B|BunsetuPositionType=SEM_HEAD|LUWBILabel=B|LUWPOS=名詞-普通名詞-一般|SpaceAfter=No\n",
      "32\tの\tの\tADP\t助詞-格助詞\t_\t31\tcase\t_\tBunsetuBILabel=I|BunsetuPositionType=SYN_HEAD|LUWBILabel=B|LUWPOS=助詞-格助詞|SpaceAfter=No\n",
      "33\t一\t一\tNUM\t名詞-数詞\t_\t34\tnummod\t_\tBunsetuBILabel=B|BunsetuPositionType=CONT|LUWBILabel=B|LUWPOS=名詞-数詞|SpaceAfter=No\n",
      "34\tつ\tつ\tNOUN\t接尾辞-名詞的-助数詞\t_\t37\tnsubj\t_\tBunsetuBILabel=I|BunsetuPositionType=SEM_HEAD|LUWBILabel=I|LUWPOS=名詞-数詞|SpaceAfter=No\n",
      "35\tが\tが\tADP\t助詞-格助詞\t_\t34\tcase\t_\tBunsetuBILabel=I|BunsetuPositionType=SYN_HEAD|LUWBILabel=B|LUWPOS=助詞-格助詞|SpaceAfter=No\n",
      "36\tこの\tこの\tDET\t連体詞\t_\t37\tdet\t_\tBunsetuBILabel=B|BunsetuPositionType=SEM_HEAD|LUWBILabel=B|LUWPOS=連体詞|SpaceAfter=No\n",
      "37\tスクープ\tスクープ\tNOUN\t名詞-普通名詞-サ変可能\t_\t0\troot\t_\tBunsetuBILabel=B|BunsetuPositionType=ROOT|LUWBILabel=B|LUWPOS=名詞-普通名詞-一般|SpaceAfter=No\n",
      "38\tで\tだ\tAUX\t助動詞-助動詞-ダ\t_\t37\tcop\t_\tBunsetuBILabel=I|BunsetuPositionType=FUNC|LUWBILabel=B|LUWPOS=助動詞-五段-ラ行|SpaceAfter=No\n",
      "39\tある\tある\tVERB\t動詞-非自立可能-五段-ラ行\t_\t38\tfixed\t_\tBunsetuBILabel=I|BunsetuPositionType=SYN_HEAD|LUWBILabel=I|LUWPOS=助動詞-五段-ラ行|SpaceAfter=No\n",
      "40\t。\t。\tPUNCT\t補助記号-句点\t_\t37\tpunct\t_\tBunsetuBILabel=I|BunsetuPositionType=CONT|LUWBILabel=B|LUWPOS=補助記号-句点|SpaceAfter=Yes\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(train_data[0].serialize())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YEIoCcOXtyBu"
   },
   "source": [
    "## Task 3. Make the collate function (0.25 points)\n",
    "\n",
    "To make a mini-batch for the character-level, unfold each sentence and concatenate it into one padded 2-d tensor.\n",
    "\n",
    "For example, this input:\n",
    "\n",
    "```\n",
    "[[tensor([46, 16,  2], device='cuda:0'),\n",
    "  tensor([25, 18,  3, 10,  5,  2,  5], device='cuda:0'),\n",
    "  tensor([10, 14, 12, 12,  9], device='cuda:0'),\n",
    "  tensor([6, 8, 7, 4], device='cuda:0'),\n",
    "  tensor([ 5,  2,  7,  2, 17, 21, 14,  2,  5,  6,  3,  7,  6], device='cuda:0'),\n",
    "  tensor([ 4,  5,  4, 10,  8,  6,  3,  7,  6], device='cuda:0'),\n",
    "  tensor([20], device='cuda:0')],\n",
    " [tensor([32,  3,  5,  6,  4], device='cuda:0'),\n",
    "  tensor([32, 10,  5, 17, 14,  3,  5,  5,  4], device='cuda:0'),\n",
    "  tensor([ 6,  3,  2, 11,  2], device='cuda:0'),\n",
    "  tensor([12,  9], device='cuda:0'),\n",
    "  tensor([32,  3,  5,  6,  4], device='cuda:0'),\n",
    "  tensor([36,  2,  9, 10], device='cuda:0'),\n",
    "  tensor([ 8,  8, 14,  4,  9,  8, 11], device='cuda:0'),\n",
    "  tensor([49,  2,  9,  5,  2, 17,  2,  9, 16,  2], device='cuda:0'),\n",
    "  tensor([ 6,  3, 18,  4,  9, 16,  8,  4, 11], device='cuda:0'),\n",
    "  tensor([23], device='cuda:0'),\n",
    "  tensor([13,  4,  5], device='cuda:0'),\n",
    "  tensor([ 6, 12,  4, 13,  8,  5,  4, 11], device='cuda:0'),\n",
    "  tensor([10,  2, 10,  5], device='cuda:0'),\n",
    "  tensor([2, 2, 5, 6, 2, 6], device='cuda:0'),\n",
    "  tensor([ 6,  2, 16,  2,  5,  4], device='cuda:0'),\n",
    "  tensor([ 5,  8, 15,  3,  7], device='cuda:0'),\n",
    "  tensor([19,  2], device='cuda:0'),\n",
    "  tensor([13,  4,  7,  7,  3], device='cuda:0'),\n",
    "  tensor([10, 21,  4, 16,  8,  5], device='cuda:0'),\n",
    "  tensor([15, 12, 12,  7,  2,  5], device='cuda:0'),\n",
    "  tensor([17,  2,  9, 10,  2], device='cuda:0'),\n",
    "  tensor([ 7,  4, 16,  4], device='cuda:0'),\n",
    "  tensor([13,  4,  7, 19,  2, 14, 11,  4], device='cuda:0'),\n",
    "  tensor([10, 14, 12, 12,  9,  4], device='cuda:0'),\n",
    "  tensor([8, 7, 2, 6, 8, 5, 3, 5], device='cuda:0'),\n",
    "  tensor([10,  2, 18,  6,  7,  2,  5,  6], device='cuda:0'),\n",
    "  tensor([14,  2, 18,  2], device='cuda:0'),\n",
    "  tensor([20], device='cuda:0')]]\n",
    "```\n",
    "\n",
    "should become this mini-batch:\n",
    "\n",
    "```\n",
    "tensor([[46, 16,  2,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
    "        [25, 18,  3, 10,  5,  2,  5,  0,  0,  0,  0,  0,  0],\n",
    "        [10, 14, 12, 12,  9,  0,  0,  0,  0,  0,  0,  0,  0],\n",
    "        [ 6,  8,  7,  4,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
    "        [ 5,  2,  7,  2, 17, 21, 14,  2,  5,  6,  3,  7,  6],\n",
    "        [ 4,  5,  4, 10,  8,  6,  3,  7,  6,  0,  0,  0,  0],\n",
    "        [20,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
    "        [32,  3,  5,  6,  4,  0,  0,  0,  0,  0,  0,  0,  0],\n",
    "        [32, 10,  5, 17, 14,  3,  5,  5,  4,  0,  0,  0,  0],\n",
    "        [ 6,  3,  2, 11,  2,  0,  0,  0,  0,  0,  0,  0,  0],\n",
    "        [12,  9,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
    "        [32,  3,  5,  6,  4,  0,  0,  0,  0,  0,  0,  0,  0],\n",
    "        [36,  2,  9, 10,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
    "        [ 8,  8, 14,  4,  9,  8, 11,  0,  0,  0,  0,  0,  0],\n",
    "        [49,  2,  9,  5,  2, 17,  2,  9, 16,  2,  0,  0,  0],\n",
    "        [ 6,  3, 18,  4,  9, 16,  8,  4, 11,  0,  0,  0,  0],\n",
    "        [23,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
    "        [13,  4,  5,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
    "        [ 6, 12,  4, 13,  8,  5,  4, 11,  0,  0,  0,  0,  0],\n",
    "        [10,  2, 10,  5,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
    "        [ 2,  2,  5,  6,  2,  6,  0,  0,  0,  0,  0,  0,  0],\n",
    "        [ 6,  2, 16,  2,  5,  4,  0,  0,  0,  0,  0,  0,  0],\n",
    "        [ 5,  8, 15,  3,  7,  0,  0,  0,  0,  0,  0,  0,  0],\n",
    "        [19,  2,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
    "        [13,  4,  7,  7,  3,  0,  0,  0,  0,  0,  0,  0,  0],\n",
    "        [10, 21,  4, 16,  8,  5,  0,  0,  0,  0,  0,  0,  0],\n",
    "        [15, 12, 12,  7,  2,  5,  0,  0,  0,  0,  0,  0,  0],\n",
    "        [17,  2,  9, 10,  2,  0,  0,  0,  0,  0,  0,  0,  0],\n",
    "        [ 7,  4, 16,  4,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
    "        [13,  4,  7, 19,  2, 14, 11,  4,  0,  0,  0,  0,  0],\n",
    "        [10, 14, 12, 12,  9,  4,  0,  0,  0,  0,  0,  0,  0],\n",
    "        [ 8,  7,  2,  6,  8,  5,  3,  5,  0,  0,  0,  0,  0],\n",
    "        [10,  2, 18,  6,  7,  2,  5,  6,  0,  0,  0,  0,  0],\n",
    "        [14,  2, 18,  2,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
    "        [20,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]], device='cuda:0')\n",
    "```\n",
    "\n",
    "Also save the length of each word in characters. The length tensor should have dtype `torch.long` and be on a CPU. For the input above, it should be:\n",
    "\n",
    "```\n",
    "tensor([ 3,  7,  5,  4, 13,  9,  1,  5,  9,  5,  2,  5,  4,  7, 10,  9,  1,  3,\n",
    "         8,  4,  6,  6,  5,  2,  5,  6,  6,  5,  4,  8,  6,  8,  8,  4,  1])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {
    "id": "PrV8KOlaBVXf"
   },
   "outputs": [],
   "source": [
    "def _collate_fn(batch):\n",
    "    # TODO: Padded mini-batch for characters\n",
    "    chars =  pad_sequence([each for item in batch for each in item[0]], batch_first=True)\n",
    "    # TODO: Lengths of each token in characters\n",
    "    chars_len = torch.count_nonzero(chars,dim=1)\n",
    "    \n",
    "    words = pad_sequence([item[1] for item in batch], batch_first=True)\n",
    "    words_len = torch.tensor([len(item[1]) for item in batch], dtype=torch.long)\n",
    "    upos = pad_sequence([item[2] for item in batch], batch_first=True)\n",
    "    feats = pad_sequence([item[3] for item in batch], batch_first=True)\n",
    "    return chars, chars_len, words, words_len, upos, feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {
    "id": "WwhRkrtVE1f5"
   },
   "outputs": [],
   "source": [
    "# Change the batch size to a lower value if you run out of memory\n",
    "batch_size = 32\n",
    "test_batch_size = 8\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, collate_fn=_collate_fn, shuffle=True)\n",
    "dev_dataloader = DataLoader(dev_dataset, batch_size=batch_size, collate_fn=_collate_fn, shuffle=False)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=test_batch_size, collate_fn=_collate_fn, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qpcBi5ZmtyBv"
   },
   "source": [
    "Here you can check if you did everything correctly.\n",
    "\n",
    "First, the first dimension of `chars` should be equal to the sum of all `words_len`.\n",
    "Then, the sum of all `chars_len` should be equal to the number of non-zero elements in the `chars`.\n",
    "\n",
    "If you don't see any error here then everything is probably fine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {
    "id": "wVx-Cr9mtyBw"
   },
   "outputs": [],
   "source": [
    "batch = next(iter(train_dataloader))\n",
    "\n",
    "assert batch[0].size(0) == sum(batch[3]), f\"The first dimension of chars must be equal to the sum of all elements in words_len! Got {batch[0].size()} and {sum(batch[3])} instead.\"\n",
    "assert sum(batch[1]) == batch[0].count_nonzero(), f\"The sum of all chars_len must be equal to the number of non-zero elements in chars! Got {sum(batch[1])} and {batch[0].count_nonzero()} instead.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jWFFZ_CotyBw"
   },
   "source": [
    "## Task 4. Create a character-level LSTM encoder (2.5 points)\n",
    "\n",
    "Here, you will need to create a character-level encoder. It will take the characters for each token as an input and will produce a single vector of each token.\n",
    "\n",
    "First, define the embedding layer with the number of embeddings equal to `vocab_size` and embedding dim to `emb_dim`. Also, specify the padding index, which is `0` in our case.\n",
    "\n",
    "Next, define a bidirectional LSTM layer. It should have the hidden size equal to `hid_dim` and one layer. Also, specify `batch_first=True`. \n",
    "\n",
    "Finally, add a dropout layer with the dropout probability of `0.5`.\n",
    "\n",
    "In the forward method, first encode the input `chars` with the embedding layer. Then, apply dropout on the embeddings.\n",
    "\n",
    "After that, pack the embedded inputs with `pack_padded_sequence` function. Make sure to specify `batch_first=True` and `enforce_sorted=False`. Pass the packed inputs to the LSTM layer.\n",
    "\n",
    "Take the last hidden state of the input. Don't forget to concatenate the hidden dimensions since we have a bidirectional LSTM (use [`torch.cat`](https://pytorch.org/docs/stable/generated/torch.cat.html?highlight=cat#torch.cat) for that). Refer to the [`nn.LSTM`](https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html?highlight=lstm#torch.nn.LSTM) documentation to get more information about the inputs and outputs.\n",
    "\n",
    "In the end, you model should output a 2-d tensor where the first dimension is the same as in `chars` and the second dimension is `hid_dim * 2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {
    "id": "pbmWKRFytyBw"
   },
   "outputs": [],
   "source": [
    "class CharLSTM(nn.Module):\n",
    "    def __init__(self, emb_dim, hid_dim, vocab_size):\n",
    "        super().__init__()\n",
    "        # TODO: Initialize embedding layer\n",
    "        self.emb = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n",
    "        # TODO: Initialize LSTM layer\n",
    "      \n",
    "        self.lstm = nn.LSTM(input_size=emb_dim, \n",
    "                            hidden_size=hid_dim, \n",
    "                            num_layers=1, \n",
    "                            batch_first=True, \n",
    "                            bidirectional=True)\n",
    "        # TODO: Initialize dropout layer\n",
    "        self.drop = nn.Dropout()\n",
    "        \n",
    "    def forward(self, chars, chars_len):\n",
    "        # TODO: Embed the inputs and apply dropout\n",
    "        x = self.drop(self.emb(chars))\n",
    "        # TODO: Pack the sequence\n",
    "        x_packed = pack_padded_sequence(x,lengths=chars_len.cpu(),batch_first=True,enforce_sorted=False)\n",
    "        # TODO: Run LSTM layer on the packed and get the final hidden state for each element in the sequence\n",
    "        out,(hidden, cell)= self.lstm(x_packed)\n",
    "        # TODO: Concatenate the hidden dimensions from each LSTM direction\n",
    "        output = torch.cat((hidden[-2, :, :], hidden[-1, :, :]), dim=1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UxujUAmptyBw"
   },
   "source": [
    "You can test your model here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {
    "id": "rY5Lrj3TtyBx"
   },
   "outputs": [],
   "source": [
    "char_lstm = CharLSTM(100, 400, len(char_vocab))\n",
    "char_lstm = char_lstm.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {
    "id": "Q9arNqk1tyBx"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input size: torch.Size([840, 12])\n",
      "Output size: torch.Size([840, 800])\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(train_dataloader))\n",
    "char_lstm.eval()\n",
    "with torch.no_grad():\n",
    "    output = char_lstm(batch[0], batch[1])\n",
    "\n",
    "print('Input size:', batch[0].size())\n",
    "print('Output size:', output.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XdpsMrq2tyBx"
   },
   "source": [
    "## Task 5. Add character-level encoder to the model (0.25 points)\n",
    "\n",
    "Here, you will have to add the `CharLSTM` module to our final model. Use `char_emb_dim`, `char_hid_dim`, and `char_vocab_size` to initialize it. \n",
    "\n",
    "Then, calculate the correct input size for the main LSTM encoder. It should be equal to the sum of the word and character embedding sizes.\n",
    "\n",
    "You don't need to change the forward method here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {
    "id": "-sxfb7xyE_X_"
   },
   "outputs": [],
   "source": [
    "class MorphClassifier(nn.Module):\n",
    "    def __init__(self, emb_dim, hid_dim, char_emb_dim, char_hid_dim, char_vocab_size, vocab_size, num_upos, num_feats):\n",
    "        super().__init__()\n",
    "        self.emb = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n",
    "        # TODO: Initialize CharLSTM\n",
    "        self.char_lstm = CharLSTM(emb_dim,char_hid_dim,char_vocab_size)\n",
    "        # TODO: Calculate the input size\n",
    "        input_size = emb_dim + char_hid_dim*2\n",
    "        self.lstm = nn.LSTM(input_size=input_size, \n",
    "                            hidden_size=hid_dim, \n",
    "                            num_layers=1, \n",
    "                            batch_first=True, \n",
    "                            bidirectional=True)\n",
    "        \n",
    "        self.upos_clf = nn.Sequential(nn.Dropout(),\n",
    "                                      nn.Linear(hid_dim * 2, hid_dim),\n",
    "                                      nn.ReLU(),\n",
    "                                      nn.Dropout(),\n",
    "                                      nn.Linear(hid_dim, num_upos))\n",
    "        \n",
    "        self.feats_clf = nn.Sequential(nn.Dropout(),\n",
    "                                       nn.Linear(hid_dim * 2, hid_dim),\n",
    "                                       nn.ReLU(),\n",
    "                                       nn.Dropout(),\n",
    "                                       nn.Linear(hid_dim, num_feats))\n",
    "        self.drop = nn.Dropout()\n",
    "\n",
    "    def forward(self, words, words_len, chars, chars_len):\n",
    "        word_x = self.drop(self.emb(words))\n",
    "        # Encode with the character-level LSTM\n",
    "        char_x = self.char_lstm(chars, chars_len.cpu())\n",
    "        # Split the sequence back into words\n",
    "        char_x = pad_sequence(char_x.split(words_len.tolist()), batch_first=True)\n",
    "        # Concatenate word and char embeddings\n",
    "        x = torch.cat([word_x, char_x], dim=2)\n",
    "\n",
    "        x_packed = pack_padded_sequence(x, words_len, batch_first=True, enforce_sorted=False)\n",
    "        hidden_packed, (_, _) = self.lstm(x_packed)\n",
    "        hidden, lens = pad_packed_sequence(hidden_packed, batch_first=True)\n",
    "\n",
    "        upos_pred = self.upos_clf(hidden)\n",
    "        feats_pred = self.feats_clf(hidden)\n",
    "\n",
    "        return upos_pred, feats_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {
    "id": "wdCOwGBLFB1M"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MorphClassifier(\n",
       "  (emb): Embedding(20179, 75, padding_idx=0)\n",
       "  (char_lstm): CharLSTM(\n",
       "    (emb): Embedding(2857, 75, padding_idx=0)\n",
       "    (lstm): LSTM(75, 400, batch_first=True, bidirectional=True)\n",
       "    (drop): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       "  (lstm): LSTM(875, 300, batch_first=True, bidirectional=True)\n",
       "  (upos_clf): Sequential(\n",
       "    (0): Dropout(p=0.5, inplace=False)\n",
       "    (1): Linear(in_features=600, out_features=300, bias=True)\n",
       "    (2): ReLU()\n",
       "    (3): Dropout(p=0.5, inplace=False)\n",
       "    (4): Linear(in_features=300, out_features=17, bias=True)\n",
       "  )\n",
       "  (feats_clf): Sequential(\n",
       "    (0): Dropout(p=0.5, inplace=False)\n",
       "    (1): Linear(in_features=600, out_features=300, bias=True)\n",
       "    (2): ReLU()\n",
       "    (3): Dropout(p=0.5, inplace=False)\n",
       "    (4): Linear(in_features=300, out_features=4, bias=True)\n",
       "  )\n",
       "  (drop): Dropout(p=0.5, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 326,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_feats = len(feats_vocab)\n",
    "num_upos = len(upos_vocab)\n",
    "num_words = len(word_vocab)\n",
    "num_chars = len(char_vocab)\n",
    "\n",
    "emb_dim = 75\n",
    "hid_dim = 300\n",
    "char_emb_dim = 100\n",
    "char_hid_dim = 400\n",
    "\n",
    "model = MorphClassifier(emb_dim, hid_dim, char_emb_dim, char_hid_dim, num_chars, num_words, num_upos, num_feats)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {
    "id": "vQmvekN8LMys"
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "optimizer = torch.optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "21dRwbXmtyBy"
   },
   "source": [
    "## Task 6. Train and evaluate your model (0.5 points)\n",
    "\n",
    "Train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {
    "id": "w3qLPDhhL0Mv"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50 | Train Loss: 0.7688503351686228 | Dev Loss: 0.2451505959033966\n",
      "Loss decreased (inf -> 0.2451505959033966). Saving the model to model_char_ja_gsd_best.pt...\n",
      "Epoch 2/50 | Train Loss: 0.2609578473535598 | Dev Loss: 0.16550418734550476\n",
      "Loss decreased (0.2451505959033966 -> 0.16550418734550476). Saving the model to model_char_ja_gsd_best.pt...\n",
      "Epoch 3/50 | Train Loss: 0.19330683229196124 | Dev Loss: 0.13860346376895905\n",
      "Loss decreased (0.16550418734550476 -> 0.13860346376895905). Saving the model to model_char_ja_gsd_best.pt...\n",
      "Epoch 4/50 | Train Loss: 0.15962970094982856 | Dev Loss: 0.11537754535675049\n",
      "Loss decreased (0.13860346376895905 -> 0.11537754535675049). Saving the model to model_char_ja_gsd_best.pt...\n",
      "Epoch 5/50 | Train Loss: 0.1359239207133988 | Dev Loss: 0.11502741277217865\n",
      "Loss decreased (0.11537754535675049 -> 0.11502741277217865). Saving the model to model_char_ja_gsd_best.pt...\n",
      "Epoch 6/50 | Train Loss: 0.11872395131383007 | Dev Loss: 0.10304304957389832\n",
      "Loss decreased (0.11502741277217865 -> 0.10304304957389832). Saving the model to model_char_ja_gsd_best.pt...\n",
      "Epoch 7/50 | Train Loss: 0.10447045580833746 | Dev Loss: 0.09921225160360336\n",
      "Loss decreased (0.10304304957389832 -> 0.09921225160360336). Saving the model to model_char_ja_gsd_best.pt...\n",
      "Epoch 8/50 | Train Loss: 0.09431272826043728 | Dev Loss: 0.09806157648563385\n",
      "Loss decreased (0.09921225160360336 -> 0.09806157648563385). Saving the model to model_char_ja_gsd_best.pt...\n",
      "Epoch 9/50 | Train Loss: 0.08463841433978189 | Dev Loss: 0.09840977191925049\n",
      "Epoch 10/50 | Train Loss: 0.07757221411795638 | Dev Loss: 0.09467289596796036\n",
      "Loss decreased (0.09806157648563385 -> 0.09467289596796036). Saving the model to model_char_ja_gsd_best.pt...\n",
      "Epoch 11/50 | Train Loss: 0.07069116048683408 | Dev Loss: 0.09974707663059235\n",
      "Epoch 12/50 | Train Loss: 0.06527223630188816 | Dev Loss: 0.0973811149597168\n",
      "Epoch 13/50 | Train Loss: 0.057433042051565596 | Dev Loss: 0.10381093621253967\n",
      "Epoch 14/50 | Train Loss: 0.054519083704883696 | Dev Loss: 0.09745442122220993\n",
      "Epoch 15/50 | Train Loss: 0.05037809604972736 | Dev Loss: 0.09935228526592255\n",
      "Epoch 16/50 | Train Loss: 0.04697280555828664 | Dev Loss: 0.10108139365911484\n",
      "Epoch 17/50 | Train Loss: 0.04428703860459824 | Dev Loss: 0.10244211554527283\n",
      "Epoch 18/50 | Train Loss: 0.040439411525812624 | Dev Loss: 0.10879537463188171\n",
      "Epoch 19/50 | Train Loss: 0.038101558771608104 | Dev Loss: 0.10447393357753754\n",
      "Epoch 20/50 | Train Loss: 0.0358737496768727 | Dev Loss: 0.11367086321115494\n",
      "Epoch 21/50 | Train Loss: 0.03520609782292293 | Dev Loss: 0.1142115443944931\n",
      "Epoch 22/50 | Train Loss: 0.03243498780608717 | Dev Loss: 0.11356279999017715\n",
      "Epoch 23/50 | Train Loss: 0.03110813339371487 | Dev Loss: 0.12082131206989288\n",
      "Epoch 24/50 | Train Loss: 0.03059851961437933 | Dev Loss: 0.12468426674604416\n",
      "Epoch 25/50 | Train Loss: 0.028801348414356352 | Dev Loss: 0.13037557899951935\n",
      "Epoch 26/50 | Train Loss: 0.027372623460864588 | Dev Loss: 0.13564346730709076\n",
      "Epoch 27/50 | Train Loss: 0.025296636296613183 | Dev Loss: 0.13093531131744385\n",
      "Epoch 28/50 | Train Loss: 0.024604158703558046 | Dev Loss: 0.12498827278614044\n",
      "Epoch 29/50 | Train Loss: 0.023015645834115837 | Dev Loss: 0.13438019156455994\n",
      "Epoch 30/50 | Train Loss: 0.024106664355523985 | Dev Loss: 0.1313353180885315\n",
      "Epoch 31/50 | Train Loss: 0.02297060953545894 | Dev Loss: 0.13353103399276733\n",
      "Epoch 32/50 | Train Loss: 0.021120341115407813 | Dev Loss: 0.12745173275470734\n",
      "Epoch 33/50 | Train Loss: 0.020111683806682604 | Dev Loss: 0.1383650004863739\n",
      "Epoch 34/50 | Train Loss: 0.02142685031459342 | Dev Loss: 0.1368933916091919\n",
      "Epoch 35/50 | Train Loss: 0.02020529708171862 | Dev Loss: 0.13343192636966705\n",
      "Epoch 36/50 | Train Loss: 0.019930086524238413 | Dev Loss: 0.1388774812221527\n",
      "Epoch 37/50 | Train Loss: 0.019141251145444845 | Dev Loss: 0.13874107599258423\n",
      "Epoch 38/50 | Train Loss: 0.017997368428502147 | Dev Loss: 0.14135028421878815\n",
      "Epoch 39/50 | Train Loss: 0.017914398762974803 | Dev Loss: 0.1413837969303131\n",
      "Epoch 40/50 | Train Loss: 0.018676800965184 | Dev Loss: 0.13750635087490082\n",
      "Epoch 41/50 | Train Loss: 0.018505325144772076 | Dev Loss: 0.14394041895866394\n",
      "Epoch 42/50 | Train Loss: 0.01746525483972886 | Dev Loss: 0.138320192694664\n",
      "Epoch 43/50 | Train Loss: 0.01689328866846421 | Dev Loss: 0.14558443427085876\n",
      "Epoch 44/50 | Train Loss: 0.01684399751516489 | Dev Loss: 0.14680472016334534\n",
      "Epoch 45/50 | Train Loss: 0.01578970947956068 | Dev Loss: 0.14861930906772614\n",
      "Epoch 46/50 | Train Loss: 0.01570891255167275 | Dev Loss: 0.14660939574241638\n",
      "Epoch 47/50 | Train Loss: 0.015777066821965696 | Dev Loss: 0.1557806134223938\n",
      "Epoch 48/50 | Train Loss: 0.014931343259854554 | Dev Loss: 0.15193979442119598\n",
      "Epoch 49/50 | Train Loss: 0.014640547031730549 | Dev Loss: 0.1510305404663086\n",
      "Epoch 50/50 | Train Loss: 0.014259616713718053 | Dev Loss: 0.1477198600769043\n"
     ]
    }
   ],
   "source": [
    "num_iters = 50\n",
    "best_loss = float('inf')\n",
    "save_path = Path(f'model_char_{treebank_short}_best.pt')\n",
    "\n",
    "for i in range(num_iters):\n",
    "    model.train()\n",
    "    iter_loss = 0\n",
    "    for chars, chars_len, words, words_len, upos, feats in train_dataloader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        upos_pred, feats_pred = model(words, words_len, chars, chars_len)\n",
    "\n",
    "        loss = criterion(upos_pred.flatten(0, 1), upos.flatten())\n",
    "        loss += criterion(feats_pred.flatten(0, 1), feats.flatten())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        iter_loss += loss\n",
    "    \n",
    "    model.eval()\n",
    "    dev_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for chars, chars_len, words, words_len, upos, feats in dev_dataloader:\n",
    "            upos_pred, feats_pred = model(words, words_len, chars, chars_len)\n",
    "\n",
    "            loss = criterion(upos_pred.flatten(0, 1), upos.flatten())\n",
    "            loss += criterion(feats_pred.flatten(0, 1), feats.flatten())\n",
    "\n",
    "            dev_loss += loss\n",
    "\n",
    "    dev_loss = dev_loss.item() / len(dev_dataloader)\n",
    "    print(f\"Epoch {i+1}/{num_iters} | Train Loss: {iter_loss.item() / len(train_dataloader)} | \" +\n",
    "          f\"Dev Loss: {dev_loss}\")\n",
    "    \n",
    "    if dev_loss < best_loss:\n",
    "        print(f'Loss decreased ({best_loss} -> {dev_loss}). Saving the model to {save_path}...')\n",
    "        best_loss = dev_loss \n",
    "        torch.save(model, save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RZFUENh-tyBz"
   },
   "source": [
    "Load the best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {
    "id": "P8Fus_uwf6Xy"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MorphClassifier(\n",
       "  (emb): Embedding(20179, 75, padding_idx=0)\n",
       "  (char_lstm): CharLSTM(\n",
       "    (emb): Embedding(2857, 75, padding_idx=0)\n",
       "    (lstm): LSTM(75, 400, batch_first=True, bidirectional=True)\n",
       "    (drop): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       "  (lstm): LSTM(875, 300, batch_first=True, bidirectional=True)\n",
       "  (upos_clf): Sequential(\n",
       "    (0): Dropout(p=0.5, inplace=False)\n",
       "    (1): Linear(in_features=600, out_features=300, bias=True)\n",
       "    (2): ReLU()\n",
       "    (3): Dropout(p=0.5, inplace=False)\n",
       "    (4): Linear(in_features=300, out_features=17, bias=True)\n",
       "  )\n",
       "  (feats_clf): Sequential(\n",
       "    (0): Dropout(p=0.5, inplace=False)\n",
       "    (1): Linear(in_features=600, out_features=300, bias=True)\n",
       "    (2): ReLU()\n",
       "    (3): Dropout(p=0.5, inplace=False)\n",
       "    (4): Linear(in_features=300, out_features=4, bias=True)\n",
       "  )\n",
       "  (drop): Dropout(p=0.5, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 330,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = torch.load(save_path)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sQrvV1NYtyBz"
   },
   "source": [
    "Print out the predictions for the first eight sentences in the test set. \n",
    "\n",
    "Report the cases where your model made a mistake. Why do you think it made it?\n",
    "\n",
    "__YOUR ANSWER BELOW:__\n",
    "\n",
    "**(A) :**\n",
    "Modal in UPOS **AUX** sometimes count as **VERB**, for example \"し\" in 2nd sentence. Because in the usage method is not too different from the main verbs and they are thus tagged VERB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {
    "id": "k6zu3OBNOF0s"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token\tupos_pred\tupos_real\tfeats_pred\tfeats_real\n",
      "\n",
      "これ\tPRON\tPRON\t_\t_\n",
      "に\tADP\tADP\t_\t_\n",
      "不快\tNOUN\tNOUN\t_\t_\n",
      "感\tNOUN\tNOUN\t_\t_\n",
      "を\tADP\tADP\t_\t_\n",
      "示す\tVERB\tVERB\t_\t_\n",
      "住民\tNOUN\tNOUN\t_\t_\n",
      "は\tADP\tADP\t_\t_\n",
      "い\tVERB\tVERB\t_\t_\n",
      "まし\tAUX\tAUX\t_\t_\n",
      "た\tAUX\tAUX\t_\t_\n",
      "が\tSCONJ\tSCONJ\t_\t_\n",
      ",\tPUNCT\tPUNCT\t_\t_\n",
      "現在\tADV\tADV\t_\t_\n",
      ",\tPUNCT\tPUNCT\t_\t_\n",
      "表立っ\tVERB\tVERB\t_\t_\n",
      "て\tSCONJ\tSCONJ\t_\t_\n",
      "反対\tNOUN\tNOUN\t_\t_\n",
      "や\tADP\tADP\t_\t_\n",
      "抗議\tNOUN\tNOUN\t_\t_\n",
      "の\tADP\tADP\t_\t_\n",
      "声\tNOUN\tNOUN\t_\t_\n",
      "を\tADP\tADP\t_\t_\n",
      "挙げ\tVERB\tVERB\t_\t_\n",
      "て\tSCONJ\tSCONJ\t_\t_\n",
      "いる\tVERB\tVERB\t_\t_\n",
      "住民\tNOUN\tNOUN\t_\t_\n",
      "は\tADP\tADP\t_\t_\n",
      "い\tVERB\tVERB\t_\t_\n",
      "ない\tSCONJ\tSCONJ\tPolarity=Neg\tPolarity=Neg\n",
      "よう\tAUX\tAUX\t_\t_\n",
      "です\tAUX\tAUX\t_\t_\n",
      "。\tPUNCT\tPUNCT\t_\t_\n",
      "\n",
      "---\n",
      "\n",
      "token\tupos_pred\tupos_real\tfeats_pred\tfeats_real\n",
      "\n",
      "幸福\tNOUN\tNOUN\t_\t_\n",
      "の\tADP\tADP\t_\t_\n",
      "科学\tNOUN\tNOUN\t_\t_\n",
      "側\tNOUN\tNOUN\t_\t_\n",
      "から\tADP\tADP\t_\t_\n",
      "は\tADP\tADP\t_\t_\n",
      ",\tPUNCT\tPUNCT\t_\t_\n",
      "特に\tADV\tADV\t_\t_\n",
      "どう\tADV\tADV\t_\t_\n",
      "し\tVERB\tAUX\t_\t_\n",
      "て\tSCONJ\tSCONJ\t_\t_\n",
      "ほしい\tAUX\tAUX\t_\t_\n",
      "と\tADP\tADP\t_\t_\n",
      "いう\tVERB\tVERB\t_\t_\n",
      "要望\tNOUN\tNOUN\t_\t_\n",
      "は\tADP\tADP\t_\t_\n",
      "いただい\tVERB\tVERB\t_\t_\n",
      "て\tSCONJ\tSCONJ\t_\t_\n",
      "い\tVERB\tVERB\t_\t_\n",
      "ませ\tAUX\tAUX\t_\t_\n",
      "ん\tSCONJ\tSCONJ\tPolarity=Neg\tPolarity=Neg\n",
      "。\tPUNCT\tPUNCT\t_\t_\n",
      "\n",
      "---\n",
      "\n",
      "token\tupos_pred\tupos_real\tfeats_pred\tfeats_real\n",
      "\n",
      "星取り\tNOUN\tNOUN\t_\t_\n",
      "参加\tNOUN\tNOUN\t_\t_\n",
      "は\tADP\tADP\t_\t_\n",
      "当然\tADV\tADJ\t_\t_\n",
      "と\tADP\tADP\t_\t_\n",
      "さ\tVERB\tVERB\t_\t_\n",
      "れ\tAUX\tAUX\t_\t_\n",
      ",\tPUNCT\tPUNCT\t_\t_\n",
      "不\tNOUN\tNOUN\tPolarity=Neg\tPolarity=Neg\n",
      "参加\tNOUN\tNOUN\t_\t_\n",
      "は\tADP\tADP\t_\t_\n",
      "白眼\tNOUN\tNOUN\t_\t_\n",
      "視\tNOUN\tNOUN\t_\t_\n",
      "さ\tAUX\tAUX\t_\t_\n",
      "れる\tAUX\tAUX\t_\t_\n",
      "。\tPUNCT\tPUNCT\t_\t_\n",
      "\n",
      "---\n",
      "\n",
      "token\tupos_pred\tupos_real\tfeats_pred\tfeats_real\n",
      "\n",
      "室長\tNOUN\tNOUN\t_\t_\n",
      "の\tADP\tADP\t_\t_\n",
      "対応\tNOUN\tNOUN\t_\t_\n",
      "に\tADP\tADP\t_\t_\n",
      "は\tADP\tADP\t_\t_\n",
      "終始\tNOUN\tADV\t_\t_\n",
      "誠実\tADJ\tNOUN\t_\t_\n",
      "さ\tPART\tPART\t_\t_\n",
      "が\tADP\tADP\t_\t_\n",
      "感じ\tVERB\tVERB\t_\t_\n",
      "られ\tAUX\tAUX\t_\t_\n",
      "た\tAUX\tAUX\t_\t_\n",
      "。\tPUNCT\tPUNCT\t_\t_\n",
      "\n",
      "---\n",
      "\n",
      "token\tupos_pred\tupos_real\tfeats_pred\tfeats_real\n",
      "\n",
      "多く\tNOUN\tNOUN\t_\t_\n",
      "の\tADP\tADP\t_\t_\n",
      "女性\tNOUN\tNOUN\t_\t_\n",
      "が\tADP\tADP\t_\t_\n",
      "生理\tNOUN\tNOUN\t_\t_\n",
      "の\tADP\tADP\t_\t_\n",
      "こと\tNOUN\tNOUN\t_\t_\n",
      "で\tADP\tADP\t_\t_\n",
      "悩ん\tVERB\tVERB\t_\t_\n",
      "で\tSCONJ\tSCONJ\t_\t_\n",
      "い\tVERB\tVERB\t_\t_\n",
      "ます\tAUX\tAUX\t_\t_\n",
      "。\tPUNCT\tPUNCT\t_\t_\n",
      "\n",
      "---\n",
      "\n",
      "token\tupos_pred\tupos_real\tfeats_pred\tfeats_real\n",
      "\n",
      "先生\tNOUN\tNOUN\t_\t_\n",
      "の\tADP\tADP\t_\t_\n",
      "理想\tNOUN\tNOUN\t_\t_\n",
      "は\tADP\tADP\t_\t_\n",
      "限りなく\tADJ\tADJ\t_\t_\n",
      "高い\tADJ\tADJ\t_\t_\n",
      "。\tPUNCT\tPUNCT\t_\t_\n",
      "\n",
      "---\n",
      "\n",
      "token\tupos_pred\tupos_real\tfeats_pred\tfeats_real\n",
      "\n",
      "それ\tPRON\tPRON\t_\t_\n",
      "は\tADP\tADP\t_\t_\n",
      "兎も角\tADV\tADV\t_\t_\n",
      ",\tPUNCT\tPUNCT\t_\t_\n",
      "私\tPRON\tPRON\t_\t_\n",
      "も\tADP\tADP\t_\t_\n",
      "明日\tPROPN\tNOUN\t_\t_\n",
      "の\tADP\tADP\t_\t_\n",
      "社説\tNOUN\tNOUN\t_\t_\n",
      "を\tADP\tADP\t_\t_\n",
      "楽しみ\tNOUN\tNOUN\t_\t_\n",
      "に\tADP\tADP\t_\t_\n",
      "し\tVERB\tVERB\t_\t_\n",
      "て\tSCONJ\tSCONJ\t_\t_\n",
      "おり\tVERB\tVERB\t_\t_\n",
      "ます\tAUX\tAUX\t_\t_\n",
      "。\tPUNCT\tPUNCT\t_\t_\n",
      "\n",
      "---\n",
      "\n",
      "token\tupos_pred\tupos_real\tfeats_pred\tfeats_real\n",
      "\n",
      "そう\tADV\tADV\t_\t_\n",
      "だっ\tAUX\tAUX\t_\t_\n",
      "たら\tAUX\tAUX\t_\t_\n",
      "いい\tAUX\tADJ\t_\t_\n",
      "なあ\tPART\tPART\t_\t_\n",
      "と\tADP\tADP\t_\t_\n",
      "は\tADP\tADP\t_\t_\n",
      "思い\tVERB\tVERB\t_\t_\n",
      "ます\tAUX\tAUX\t_\t_\n",
      "が\tSCONJ\tSCONJ\t_\t_\n",
      ",\tPUNCT\tPUNCT\t_\t_\n",
      "日本\tPROPN\tPROPN\t_\t_\n",
      "学術\tNOUN\tNOUN\t_\t_\n",
      "会議\tNOUN\tNOUN\t_\t_\n",
      "の\tADP\tADP\t_\t_\n",
      "会長\tNOUN\tNOUN\t_\t_\n",
      "談話\tNOUN\tNOUN\t_\t_\n",
      "に\tADP\tADP\t_\t_\n",
      "つい\tVERB\tVERB\t_\t_\n",
      "て\tSCONJ\tSCONJ\t_\t_\n",
      "“\tPUNCT\tPUNCT\t_\t_\n",
      "当会\tNOUN\tNOUN\t_\t_\n",
      "で\tADP\tADP\t_\t_\n",
      "は\tADP\tADP\t_\t_\n",
      ",\tPUNCT\tPUNCT\t_\t_\n",
      "標記\tNOUN\tNOUN\t_\t_\n",
      "の\tADP\tADP\t_\t_\n",
      "件\tNOUN\tNOUN\t_\t_\n",
      "に\tADP\tADP\t_\t_\n",
      "つい\tVERB\tVERB\t_\t_\n",
      "て\tSCONJ\tSCONJ\t_\t_\n",
      ",\tPUNCT\tPUNCT\t_\t_\n",
      "以下\tNOUN\tNOUN\t_\t_\n",
      "の\tADP\tADP\t_\t_\n",
      "よう\tNOUN\tNOUN\t_\t_\n",
      "に\tAUX\tAUX\t_\t_\n",
      "考え\tVERB\tVERB\t_\t_\n",
      "ます\tAUX\tAUX\t_\t_\n",
      "。\tPUNCT\tPUNCT\t_\t_\n",
      "”\tPUNCT\tPUNCT\t_\t_\n",
      "\n",
      "---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_dataloader = DataLoader(test_dataset, batch_size=test_batch_size, collate_fn=_collate_fn, shuffle=False)\n",
    "test_batch = next(iter(test_dataloader))\n",
    "test_data_batch = test_data[:test_batch_size]\n",
    "test_data_batch\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    upos_pred, feats_preds = model(test_batch[2], test_batch[3], test_batch[0], test_batch[1])\n",
    "    upos_ids = torch.argmax(torch.softmax(upos_pred, dim=2), dim=2)\n",
    "    feats_ids = torch.argmax(torch.softmax(feats_preds, dim=2), dim=2)\n",
    "    for i, ids in enumerate(upos_ids):\n",
    "        print('token\\tupos_pred\\tupos_real\\tfeats_pred\\tfeats_real\\n')\n",
    "        sent_len = len(test_data_batch[i])\n",
    "        tokens = [token[\"form\"] for token in test_data_batch[i]]\n",
    "        upos_real = [token[\"upos\"] for token in test_data_batch[i]]\n",
    "        feats_real = [\"|\".join([k + '=' + v for k, v in token[\"feats\"].items()]) if token[\"feats\"] else \"_\"\n",
    "                      for token in test_data_batch[i]]\n",
    "        upos_pred = upos_vocab.lookup_tokens(ids.tolist()[:sent_len])\n",
    "        feats_pred = feats_vocab.lookup_tokens(feats_ids[i].tolist()[:sent_len])\n",
    "\n",
    "        for token, upos, u_real, feats, f_real in zip(tokens, upos_pred, upos_real, feats_pred, feats_real):\n",
    "            print('\\t'.join([token, upos, u_real, feats, f_real]))\n",
    "        print('\\n---\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FwgziEd7tyB0"
   },
   "source": [
    "Evaluate your model on the test dataset and report the scores for UPOS and UFeats.\n",
    "\n",
    "(Optional) Did this model perform better then the one that you trained during the practice session. Compare the scores.\n",
    "\n",
    "How does your model compare to the other models in [CoNLL 2018 Shared Task](http://universaldependencies.org/conll18/results.html)?\n",
    "\n",
    "__YOUR ANSWER BELOW:__\n",
    "\n",
    "**(A) :** Comparing my model to others model in CoNLL 2018 Shared Task, my result in UPOS and Ufeats is better than 1st rank.\n",
    "\n",
    "In UPOS, my result is **97.37**, the 1st rank in CoNLL 2018 Shared Task is **92.97**.\n",
    "\n",
    "In UFeats, my result is **99.98**, the 1st rank in CoNLL 2018 Shared Task is **94.52**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {
    "id": "fB_3wUvplgoj"
   },
   "outputs": [],
   "source": [
    "test_dataloader = DataLoader(test_dataset, batch_size=test_batch_size, collate_fn=_collate_fn, shuffle=False)\n",
    "test_token_list = []\n",
    "with torch.no_grad():\n",
    "    for batch_id, (chars, chars_len, words, words_len, upos, feats) in enumerate(test_dataloader):\n",
    "        upos_pred, feats_preds = model(words, words_len, chars, chars_len)\n",
    "        upos_ids = torch.argmax(torch.softmax(upos_pred, dim=2), dim=2)\n",
    "        feats_ids = torch.argmax(torch.softmax(feats_preds, dim=2), dim=2)\n",
    "        for i, ids in enumerate(upos_ids):\n",
    "            current_id = (batch_id * test_batch_size) + i\n",
    "            sent_len = len(test_data[current_id])\n",
    "            token_ids = [token[\"id\"] for token in test_data[current_id]]\n",
    "            tokens = [token[\"form\"] for token in test_data[current_id]]\n",
    "            upos_real = [token[\"upos\"] for token in test_data[current_id]]\n",
    "            feats_real = [\"|\".join([k + '=' + v for k, v in token[\"feats\"].items()]) if token[\"feats\"] else \"_\"\n",
    "                          for token in test_data[current_id]]\n",
    "            upos_pred = upos_vocab.lookup_tokens(ids.tolist()[:sent_len])\n",
    "            feats_pred = feats_vocab.lookup_tokens(feats_ids[i].tolist()[:sent_len])\n",
    "\n",
    "            token_list = []\n",
    "            for token_id, token, upos, feats in zip(token_ids, tokens, upos_pred, feats_pred):\n",
    "                head = token_id - 1 if isinstance(token_id, int) else None\n",
    "                token_list.append({\"id\": token_id, \n",
    "                                   \"form\": token, \n",
    "                                   \"lemma\": None, \n",
    "                                   \"upos\": upos,\n",
    "                                   \"xpos\": None,\n",
    "                                   \"feats\": feats,\n",
    "                                   \"head\": head,\n",
    "                                   \"deprel\": \"root\" if head == 0 else None,\n",
    "                                   \"deps\": None,\n",
    "                                   \"misc\": None})\n",
    "            test_token_list.append(TokenList(token_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {
    "id": "HTqztMgYtyB0"
   },
   "outputs": [],
   "source": [
    "with open(Path(f'{treebank_short}-ud-test.pred.conllu'), 'w', encoding='utf-8') as f:\n",
    "    f.write(''.join([sent.serialize() for sent in test_token_list]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uw0F8KIutyB1"
   },
   "source": [
    "Don't forget to change the file names to the correct ones for your language. \n",
    "\n",
    "The evaluation script can be found here: http://universaldependencies.org/conll18/conll18_ud_eval.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {
    "id": "SSlhzW-1tyB1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metric     | Precision |    Recall |  F1 Score | AligndAcc\n",
      "-----------+-----------+-----------+-----------+-----------\n",
      "Tokens     |    100.00 |    100.00 |    100.00 |\n",
      "Sentences  |    100.00 |    100.00 |    100.00 |\n",
      "Words      |    100.00 |    100.00 |    100.00 |\n",
      "UPOS       |     97.37 |     97.37 |     97.37 |     97.37\n",
      "XPOS       |      0.00 |      0.00 |      0.00 |      0.00\n",
      "UFeats     |     99.98 |     99.98 |     99.98 |     99.98\n",
      "AllTags    |      0.00 |      0.00 |      0.00 |      0.00\n",
      "Lemmas     |      0.00 |      0.00 |      0.00 |      0.00\n",
      "UAS        |     34.69 |     34.69 |     34.69 |     34.69\n",
      "LAS        |      0.02 |      0.02 |      0.02 |      0.02\n",
      "CLAS       |      0.55 |      0.04 |      0.08 |      0.04\n",
      "MLAS       |      0.18 |      0.01 |      0.03 |      0.01\n",
      "BLEX       |      0.00 |      0.00 |      0.00 |      0.00\n"
     ]
    }
   ],
   "source": [
    "!python conll18_ud_eval.py -v ./ud-treebanks-v2.9/UD_Japanese-GSD/ja_gsd-ud-test.conllu ./ja_gsd-ud-test.pred.conllu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of Homework 7 Release.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "venv_nlp",
   "language": "python",
   "name": "venv_nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
