{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 2. Text Representations\n",
    "\n",
    "After completing this Lab, you will be able to:\n",
    "- Read the dataset\n",
    "- Preprocess the dataset\n",
    "- Build TF-IDF text representations\n",
    "- Use TF-IDF to find similar texts\n",
    "- Use TF-IDF features for text classification\n",
    "- (Optional) Use TF-IDF for text summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from string import punctuation\n",
    "from collections import Counter\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nltk.tokenize import word_tokenize\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to work with [MPST (Movie Plot Synapses with Tags)](https://ritual.uh.edu/mpst-2018/) dataset. This dataset consists of plot symopses for 14,828 movies, each plot having a set of genre tags. You can also [preview the dataset on Kaggle](https://www.kaggle.com/cryptexcode/mpst-movie-plot-synopses-with-tags).\n",
    "\n",
    "The dataset is in the CSV format.\n",
    "\n",
    "Load the dataset into pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('mpst_full_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As usual, we need to implement some preprocessing pipeline. For now, we will tokenize the texts, lowercase it, and remove the punctuation. Optionally, we can also lemmatize the texts but we will skip this step for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "373b28de4bdfbfd742165fb8e6cb76ad",
     "grade": false,
     "grade_id": "cell-2bb1bb7c68e7fe6e",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def preprocess(texts):\n",
    "    # YOUR CODE STARTS HERE\n",
    "    \n",
    "    # YOUR CODE ENDS HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess the plot synopses and add them to our pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_synopsis = preprocess(dataset['plot_synopsis'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.assign(tokenized_synopsis=pd.Series(tokenized_synopsis).values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select only the synopses used for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = dataset[dataset['split'] == 'train'].reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First step in building the text representations is to create a vocabulary for our collection of texts. In NLP, a vocabulary is very often accompanied by a mapping from words to indices. For example, if we have the following collection of texts:\n",
    "\n",
    "```[['the', 'cat', 'sat', 'on', 'the', 'mat'], ['who', 'will', 'feed', 'the', 'cat']]```\n",
    "\n",
    "the vocabulary is:\n",
    "\n",
    "```{'the', 'cat', 'sat', 'on', 'mat', 'who', 'will', 'feed'}```\n",
    "\n",
    "Then, the mapping can be as simple as:\n",
    "\n",
    "```{'the': 0, 'cat': 1, 'sat': 2, 'on': 3, 'mat': 4, 'who': 5, 'will': 6, 'feed': 7}```\n",
    "\n",
    "Finally, if we to transform our texts with the mapping, we get:\n",
    "\n",
    "```[[0, 1, 2, 3, 0, 4], [5, 6, 7, 0, 1]]```\n",
    "\n",
    "Usually, the vocabulary is ordered by the word frequency, so the words like \"the\", \"a\", \"to\" have lower indices. We can also remove some rare words from the vocabulary to reduce it's size.\n",
    "\n",
    "For our task, first create a frequency list from all the texts in the training set. Then, create a mapping from words to indices only taking the words that appear more then five times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "daa19fb5b338f187f747457b06dac774",
     "grade": false,
     "grade_id": "cell-73861f3f3548fde2",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE STARTS HERE\n",
    "\n",
    "# YOUR CODE ENDS HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(word2idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we are going to calculate the first step in the TF-IDF representation. TF stands for term frequency, and it is just number of times each word appears in each text.\n",
    "\n",
    "The frequency of the word $t$ in the document $d$ is:\n",
    "\n",
    "$$tf_{t,d} = count(t, d)$$\n",
    "\n",
    "We can also take a $\\log_{10}$ of word counts to make the numbers smaller:\n",
    "\n",
    "$$tf_{t, d} = \\log_{10}(count(t, d) + 1)$$\n",
    "\n",
    "To compute a TF matrix, we can first create an matrix of size $|V|x|D|$, where $|V|$ is the size of the vocabulary and $|D|$ is the number of documents in the collection. Then, we go through each word in each document and increase the corresponding item in the matrix by one using out vocabulary mapping.\n",
    "\n",
    "If we return to our example with the cat on the mat, $|V| = 8$ and $|D| = 2$. The final TF matrix will be:\n",
    "\n",
    "|      | d1 | d2 |\n",
    "|------|----|----|\n",
    "| the  | 2  | 1  |\n",
    "| cat  | 1  | 1  |\n",
    "| sat  | 1  | 0  |\n",
    "| on   | 1  | 0  |\n",
    "| mat  | 1  | 0  |\n",
    "| who  | 0  | 1  |\n",
    "| will | 0  | 1  |\n",
    "| feed | 0  | 1  |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2aea8d5d5d89d9794964d59d2d0925e8",
     "grade": false,
     "grade_id": "cell-87abcfe282eb1e4a",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def get_tf(texts, word2idx):\n",
    "    # YOUR CODE STARTS HERE\n",
    "    \n",
    "    # YOUR CODE ENDS HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf = get_tf(train_dataset['tokenized_synopsis'], word2idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, for the second term: IDF. IDF stands for inverse document frequency.\n",
    "\n",
    "First, we need to get the DF or the document frequency. We can do it by counting the number of documents each word appeared in. To find it, we don't need to do through the text collection again, since we can easily extract it from the TF matrix. Think on how to do it.\n",
    "\n",
    "Finally, the inverse document frequency, or IDF is a $\\log_{10}$ of number of documents divided by the document frequency:\n",
    "\n",
    "$$idf_t = \\log_{10}(\\frac{|D|}{df_t})$$\n",
    "\n",
    "Most of the times, some kind of smoothing is applied to IDF. For example, sklearn package smoothes it by artificially adding a document that contains each word in the vocabulary:\n",
    "\n",
    "$$idf_t = \\log_{10}(\\frac{1 + |D|}{1 + df_t}) + 1$$\n",
    "\n",
    "Use the smoothed IDF formula to complete the function below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "68df5522f819eb396fd06690808766ab",
     "grade": false,
     "grade_id": "cell-9176600f89ddf207",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def get_idf(tf):\n",
    "    # YOUR CODE STARTS HERE\n",
    "    \n",
    "    # YOUR CODE ENDS HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idf = get_idf(tf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last step is to assemble everything together. The final TF-IDF term is a simple multiplication of TF and IDF.\n",
    "Now, the words that are characteristic of a document will have a high TF-IDF scores and non-relevant words will have a TF-IDF score close to zero.\n",
    "\n",
    "Also, now each column represents a document. This way, we have created a document representation. But how do we use it now to find similar documents? \n",
    "\n",
    "To find the similarity between two documents, we can use the assumption that the documents are similar if their vectors have similar direction. We can quantify it by calculating a __cosine similarity__ between two vectors. From linear algerbra, we know that the dot product of two unit vectors is the cosine of the angle between them. So, the cosine of 1 means that the two vectors are proportional and cosine of 0 means that they are orthogonal. This way, the documents that have bigger cosine similarity are more similar to each other.\n",
    "\n",
    "Complete the function below. Calculate the final TF-IDF matrix and the normalize each document vector by dividing every element of a vector by the length of the vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "71d99f63cd922df13eac8897d841e158",
     "grade": false,
     "grade_id": "cell-35a32019da6566e5",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def get_tfidf(tf, idf):\n",
    "    # YOUR CODE STARTS HERE\n",
    "    \n",
    "    # YOUR CODE ENDS HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = get_tfidf(tf, idf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a fucntion that calculates the dot product between a query vector and corpus matrix, and return the indices of top-k documents sorted by the cosine similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c5b262f8c0f266525ec65d7634eb4e56",
     "grade": false,
     "grade_id": "cell-096215fd88e0f2f3",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def find_similar(query, corpus, k=5):\n",
    "    # YOUR CODE STARTS HERE\n",
    "    \n",
    "    # YOUR CODE ENDS HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to find the top-5 similar texts to the first text in out collection. If we did everything correctly, the first text should be the most similar since it have the same vector :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_similar = find_similar(tfidf[:, 0], tfidf)\n",
    "top_similar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now print the most similar plot synopses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_similar(query, top_similar):\n",
    "    print(\"Query: \")\n",
    "    print(query['title'])\n",
    "    print('---')\n",
    "    print(query['tags'])\n",
    "    print('---')\n",
    "    print(query['plot_synopsis'])\n",
    "    print('\\n\\n')\n",
    "    for i, similar in top_similar.iterrows():\n",
    "        print(f\"Similar #{i}: \")\n",
    "        print(similar['title'])\n",
    "        print('---')\n",
    "        print(similar['tags'])\n",
    "        print('---')\n",
    "        print(similar['plot_synopsis'])\n",
    "        print('\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_similar(train_dataset.loc[0], train_dataset.iloc[top_similar])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, what if we want to find similar plot synopses for another movie, not seen in the training set? Easy!\n",
    "\n",
    "We just calculate the TF matrix for the new synopsis using the same preprocessing and vocabulary mappings as for the training set. And then we just multiply it with the IDF from the training set.\n",
    "\n",
    "Go to the IMDB website and find a plot synopsis of your favorite movie. Then find the most similar movies from our train collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_title = \"Don't Look Up\"\n",
    "test_tags = ['comedy', 'drama', 'sci-fi']\n",
    "test_text = \"\"\"Kate Dibiasky, an astronomy grad student at Michigan State University, discovers the existence of an unidentified comet. Her professor, Dr. Randall Mindy, calculates that the trajectory of the comet crosses that of the Earth and that an impact will take place in about six months, killing all life in the process. Accompanied by scholar Teddy Oglethorpe, Kate and Randall travel to the White House to present their findings, but are met with apathy from U.S. President Janie Orlean and her staff, including her son, Chief of Staff Jason. The attempt to inform the population through a television program also fails, though Kate's on-camera antics go viral online. When Orlean becomes involved in a sex scandal, she announces the threat of the comet to divert attention. The news is finally spread by the media and the launch of a spaceship that can hit and divert the comet, saving the planet, is announced. However, the operation is canceled mid-flight when Peter Isherwell, a tech billionaire and prominent funder of Orlean, discovers that the comet is composed of trillions of dollars worth of precious minerals that have become scarce on Earth. The White House plans to commercially exploit the comet by crushing it to reduce its size and recovering the fragments. Kate and Teddy immediately abandon the operation in protest, while Randall submissively becomes a prominent voice in advocating for the comet's commercial opportunities, as well as starting an affair with talk show host Brie Evantee. The world becomes ideologically divided between those who demand the total destruction of the comet, those who decry unjustified alarmism, and those who deny that a comet even exists. Meanwhile, Kate returns home to Michigan and begins a relationship with a boy named Yule. After his wife June discovers his infidelity, Randall becomes angered and voices his frustrations on live television, launching into a rant criticizing Orlean's administration for downplaying the impending apocalypse and questions humanity's indifference, before leaving the operation and reconciling with Kate. Orlean and Isherwell's plan to recover the comet's materials fails, leaving them, along with a group of wealthy Americans, to flee in a spaceship designed to find the nearest Earth-like planet. However, they accidentally leave Jason behind in the process. Before leaving, Orlean offers Randall a place on the ship, but he turns her down, choosing to spend his last moments in the company of Kate, his family, Yule, and Teddy. The comet finally hits the planet, killing everyone. In a mid-credits scene set twenty-two thousand years later, the presidential ship lands on a lush alien planet. Its passengers wake up from cryogenic sleep and take a look at the surrounding environment only to immediately be attacked and killed by the planet's wild animals. In a post-credits scene, Jason is shown to have survived the extinction of life on Earth, wondering if his mother is still coming back, and documents the aftermath on his phone.\"\"\"\n",
    "\n",
    "test_dataset = {\n",
    "    'title': test_title,\n",
    "    'tags': test_tags,\n",
    "    'plot_synopsis': test_text\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tokenized = preprocess([test_text])\n",
    "test_tf = get_tf(test_tokenized, word2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tfidf = get_tfidf(test_tf, idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_top_similar = find_similar(test_tfidf[:, 0], tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_similar(test_dataset, train_dataset.iloc[test_top_similar])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luckily, we don't have to implement TF-IDF from scrach every time. sklearn package has a very convenient [`TfidfVectorizer`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html?highlight=tfidf#sklearn.feature_extraction.text.TfidfVectorizer) that is very easy to use.\n",
    "\n",
    "This time, we will initialize it with our vocabulary mappings and a dummy tokenizer so that we can compare it with our implementation. You can see what different arguments do in the documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(\n",
    "    vocabulary=word2idx, \n",
    "    tokenizer=lambda x: x, \n",
    "    preprocessor=lambda x: x, \n",
    "    token_pattern=None\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, our TF-IDF is just one line away!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = vectorizer.fit_transform(train_dataset['tokenized_synopsis'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use the [`cosine_similarity`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.cosine_similarity.html?highlight=cosine#sklearn.metrics.pairwise.cosine_similarity) function from sklearn to find similar documents.\n",
    "\n",
    "Just pay attention that sklearn implementation returns TF-IDF matrix of $|D|x|V|$, whereas our implementation returned $|V|x|D|$!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_similarity(X[0], X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare the sklearn implementation with ours. The results may be a bit different from what we had, but this is normal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_X = vectorizer.transform(test_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sklearn_top_similar = np.argsort(cosine_similarity(test_X, X)[0])[::-1][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_similar(test_dataset, train_dataset.iloc[test_sklearn_top_similar])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you can use these document representations as input features to a machine learning algorithm. This is what you are going to do in your Homework :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional exercise\n",
    "\n",
    "It is also possible to use TF-IDF to make a text summarization tool. To do that, we can split a document into sentences and calculate the TF-IDF matrix for it, just this time, we will have sentences instead of documents. \n",
    "\n",
    "Then, remember that important words have high TD-IDF score. You can use it to calculate the importance of each sentence by summing up all the scores. Finally, use some threshold to pick the most important sentences. This will be your document summary.\n",
    "\n",
    "Try to take any plot synopsis and make its summary using TF-IDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
