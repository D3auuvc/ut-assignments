{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/D3auuvc/ut-assignments/blob/main/Homeword4_CNN_Release.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q3TTb55jpC6C"
      },
      "source": [
        "## Homework 4\n",
        "### Text Classification with CNN\n",
        "\n",
        "Welcome to Homework 4! \n",
        "\n",
        "The homework contains several tasks. You can find the amount of points that you get for the correct solution in the task header. Maximum amount of points for each homework is _four_.\n",
        "\n",
        "The **grading** for each task is the following:\n",
        "- correct answer - **full points**\n",
        "- insufficient solution or solution resulting in the incorrect output - **half points**\n",
        "- no answer or completely wrong solution - **no points**\n",
        "\n",
        "Even if you don't know how to solve the task, we encourage you to write down your thoughts and progress and try to address the issues that stop you from completing the task.\n",
        "\n",
        "When working on the written tasks, try to make your answers short and accurate. Most of the times, it is possible to answer the question in 1-3 sentences.\n",
        "\n",
        "When writing code, make it readable. Choose appropriate names for your variables (`a = 'cat'` - not good, `word = 'cat'` - good). Avoid constructing lines of code longer than 100 characters (79 characters is ideal). If needed, provide the commentaries for your code, however, a good code should be easily readable without them :)\n",
        "\n",
        "Finally, all your answers should be written only by yourself. If you copy them from other sources it will be considered as an academic fraud. You can discuss the tasks with your classmates but each solution must be individual.\n",
        "\n",
        "<font color='red'>**Important!:**</font> **before sending your solution, do the `Kernel -> Restart & Run All` to ensure that all your code works.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4zZoefGFMWNO"
      },
      "outputs": [],
      "source": [
        "!pip install datasets torchmetrics --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JZprDQ1WMcrM"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from datasets import load_dataset\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchmetrics.functional import f1_score, accuracy\n",
        "import json\n",
        "from tqdm.notebook import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sBwCNa8apKQo"
      },
      "source": [
        "In this homework, you are going to work again with [Stanford Semantic Treebank](https://nlp.stanford.edu/sentiment/index.html).\n",
        "\n",
        "Only this time, we are going to split the labels into five classes, instead of two. This way, we will do a multi-class classification.\n",
        "\n",
        "Run the cell below to load the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7n9Llk_sMr6f"
      },
      "outputs": [],
      "source": [
        "sst = load_dataset(\"sst\", \"default\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gndqQVT5qlAz"
      },
      "source": [
        "Download the pretrained [GloVe 6B](https://nlp.stanford.edu/projects/glove/) word embeddings. \n",
        "\n",
        "If you are on Windows, just copy the URL into your browser, download the ZIP file and unpack it into the same folder as this notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eSlRyxW-Mtzf"
      },
      "outputs": [],
      "source": [
        "!wget https://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip glove.6B.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "code",
        "id": "0yfaS5qvMygq"
      },
      "outputs": [],
      "source": [
        "# Load the embeddings into the memory\n",
        "glove_path = 'glove.6B.300d.txt'\n",
        "glove_vecs = []\n",
        "idx2token = []\n",
        "with open(glove_path, encoding='utf-8') as f:\n",
        "    for line in tqdm(f):\n",
        "        line = line.strip().split()\n",
        "        word = line[0]\n",
        "        vec = [float(x) for x in line[1:]]\n",
        "        glove_vecs.append(vec)\n",
        "        idx2token.append(word)\n",
        "\n",
        "# Convert to tensor\n",
        "glove_vecs = torch.tensor(glove_vecs)\n",
        "\n",
        "# Put zero vector for padding and mean for unknown\n",
        "glove_vecs = torch.vstack(\n",
        "    [\n",
        "        torch.zeros(1, glove_vecs.size(1)),\n",
        "        torch.mean(glove_vecs, dim=0).unsqueeze(0),\n",
        "        glove_vecs,\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Save the embeddings in Pytorch format\n",
        "torch.save(glove_vecs, 'glove.6B.300d.pt')\n",
        "\n",
        "# Add special pad and unk tokens to the vocab\n",
        "PAD = '<pad>'\n",
        "PAD_ID = 0\n",
        "UNK = '<unk>'\n",
        "UNK_ID = 1\n",
        "\n",
        "idx2token = [PAD, UNK] + idx2token\n",
        "\n",
        "# Save the vocab\n",
        "json.dump(idx2token, open('idx2token.json', 'w', encoding='utf-8'))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Uncomment the cell below if you want to load the saved embeddings and vocabulary"
      ],
      "metadata": {
        "id": "jNQtMdombb3k"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dIJegSMzbRAA"
      },
      "outputs": [],
      "source": [
        "# PAD = '<pad>'\n",
        "# PAD_ID = 0\n",
        "# UNK = '<unk>'\n",
        "# UNK_ID = 1\n",
        "\n",
        "# glove_vecs = torch.load('glove.6B.300d.pt')\n",
        "# idx2token = json.load(open('idx2token.json', encoding='utf-8'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sR36mxtcQGQf"
      },
      "outputs": [],
      "source": [
        "# Create a reverse vocab\n",
        "token2idx = {token: idx for idx, token in enumerate(idx2token)}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l7CmNCfsUM1I"
      },
      "outputs": [],
      "source": [
        "# Set the device (GPU or CPU)\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YUD_yrB_sbNG"
      },
      "source": [
        "### Task 1. Build the dataset (1 point)\n",
        "\n",
        "The SST dataset has a sentiment label for each sentence. This label ranges from 0 to 1, 0 being very negative and 1 being very positive. During the practice session, we split the labels into a binary format, i.e. all the labels lower than 0.5 became 0 and greater than 0.5 became 1.\n",
        "\n",
        "This time, you will need to split them into five categories that have the following ranges: $(0, 0.2]$, $(0.2, 0.4]$, $(0.4, 0.6]$, $(0.6, 0.8]$, $(0.8, 1]$. They should be transformed into the labels $0$, $1$, $2$, $3$, $4$ respectively.\n",
        "\n",
        "In the end, you should have five labels:\n",
        "- 0 (very negative)\n",
        "- 1 (negative)\n",
        "- 2 (neural)\n",
        "- 3 (positive)\n",
        "- 4 (very positive)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TGUwo620OPKf"
      },
      "outputs": [],
      "source": [
        "class SSTDataset(Dataset):\n",
        "    def __init__(self, data, token2idx, max_seq_len=100, device=torch.device('cpu')):\n",
        "        super().__init__()\n",
        "        self.max_seq_len = max_seq_len\n",
        "        self.device = device\n",
        "\n",
        "        self.texts = []\n",
        "        self.labels = []\n",
        "        for item in data:\n",
        "            label = item['label']\n",
        "            tokens = [token2idx.get(token.lower(), UNK_ID) for token in item['tokens'].split('|')]\n",
        "            tokens = torch.tensor(tokens, dtype=torch.long, device=self.device)\n",
        "            self.texts.append(tokens)\n",
        "\n",
        "            # Transform the continuous label into five classes and add it to the self.labels list\n",
        "            ### YOUR CODE STARTS HERE\n",
        "            ...\n",
        "            ### YOUR CODE ENDS HERE\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        padded_text = torch.zeros(self.max_seq_len, dtype=torch.long, device=self.device)\n",
        "        text = self.texts[idx][:self.max_seq_len]\n",
        "        padded_text[:text.size(0)] = text\n",
        "        return padded_text, self.labels[idx]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the datasets."
      ],
      "metadata": {
        "id": "bON8-iE8rMif"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "umYjHzMPPshz"
      },
      "outputs": [],
      "source": [
        "train_dataset = SSTDataset(sst['train'], token2idx, max_seq_len=52, device=device)\n",
        "validation_dataset = SSTDataset(sst['validation'], token2idx, max_seq_len=52, device=device)\n",
        "test_dataset = SSTDataset(sst['test'], token2idx, max_seq_len=52, device=device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DOjQboUkAefw"
      },
      "outputs": [],
      "source": [
        "len(train_dataset), len(validation_dataset), len(test_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UECZCKiRP9j3"
      },
      "outputs": [],
      "source": [
        "train_dataloader = DataLoader(train_dataset, batch_size=50)\n",
        "validation_dataloader = DataLoader(validation_dataset, batch_size=50)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-f9cSt_ftHUV"
      },
      "source": [
        "### Task 2. Modify the Model (1 point)\n",
        "\n",
        "Since now we have five classes instead of two, you need to modify the final layer of the model to have five outputs (ref. [nn.Linear](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html?highlight=linear#torch.nn.Linear)).\n",
        "\n",
        "Also, we are going to finetune the pretrained embeddings this time, so you need to \"unfreeze\" them (ref. [nn.Embedding](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html?highlight=embedding#torch.nn.Embedding)).\n",
        "\n",
        "This task doesn't have `### YOUR CODE STARTS HERE` field, you have to find the parameters and modify them yourself."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p-HMNr2FTnyC"
      },
      "outputs": [],
      "source": [
        "class SSTClassificationModel(nn.Module):\n",
        "    def __init__(self, pretrained_emb, num_filters, kernel_sizes):\n",
        "        super().__init__()\n",
        "        self.emb = nn.Embedding.from_pretrained(pretrained_emb, padding_idx=PAD_ID, freeze=True)\n",
        "        emb_size = self.emb.weight.size(1)\n",
        "\n",
        "        self.convs = nn.ModuleList(\n",
        "            [\n",
        "                  nn.Conv1d(in_channels=emb_size, out_channels=num_filters, kernel_size=kernel_size)\n",
        "                  for kernel_size in kernel_sizes                  \n",
        "            ]\n",
        "        )\n",
        "        self.linear_out = nn.Linear(in_features=num_filters * len(kernel_sizes), out_features=2)\n",
        "        self.drop = nn.Dropout(0.5)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x size is [batch x seq_len]\n",
        "        x = self.emb(x) # [batch x seq_len x emb_dim]\n",
        "        x = x.permute(0, 2, 1) # [batch x emb_dim x seq_len]\n",
        "        xs = [torch.relu(conv(x)) for conv in self.convs] # [batch x num_filters x conv_seq_len] x num_kernels\n",
        "        xs = [torch.nn.functional.max_pool1d(x, x.size(2)).squeeze(2) for x in xs] # [batch x num_filters] x num_kernels\n",
        "        x = torch.cat(xs, dim=1) # [batch x num_filters * num_kernels]\n",
        "        x = self.drop(x)\n",
        "        x = self.linear_out(x) # [batch x 2]\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model parameters. Feel free to modify them if you'd like to."
      ],
      "metadata": {
        "id": "gbCQwNjWsOlj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VqDeDhWXXT_A"
      },
      "outputs": [],
      "source": [
        "num_filters = 100\n",
        "kernel_sizes = [3, 4, 5]\n",
        "lr = 1e-3\n",
        "\n",
        "num_iters = 100"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initalize the model."
      ],
      "metadata": {
        "id": "VZYrFOeWsW5F"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dKdE58QXXYqa"
      },
      "outputs": [],
      "source": [
        "model = SSTClassificationModel(glove_vecs, num_filters, kernel_sizes)\n",
        "model = model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fERQusefC9_w"
      },
      "outputs": [],
      "source": [
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initialize the loss and optimizer. Feel free to use different optimizer if you'd like to."
      ],
      "metadata": {
        "id": "p76KzvOCsb6W"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D3qMRf6FXwBI"
      },
      "outputs": [],
      "source": [
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train the model.\n",
        "\n",
        "__This may take around 25-30 minutes!__\n",
        "\n",
        "In the end, you should have around 0.40 accuracy.\n",
        "If the training takes too long or you have time constrains, feel free to reduce the number of epochs."
      ],
      "metadata": {
        "id": "rWGxo8kvskn1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5SQaBbe6YFol"
      },
      "outputs": [],
      "source": [
        "best_accuracy = 0.0\n",
        "for i in range(num_iters):\n",
        "    current_loss = 0\n",
        "    model.train()\n",
        "    for texts, labels in train_dataloader:\n",
        "        model.zero_grad()\n",
        "        preds = model(texts)\n",
        "        loss = loss_fn(preds, labels)\n",
        "        current_loss += loss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    avg_train_loss = current_loss.item() / len(train_dataloader)\n",
        "\n",
        "    current_loss = 0\n",
        "    current_acc = 0\n",
        "    model.eval()\n",
        "    for texts, labels in validation_dataloader:\n",
        "        with torch.no_grad():\n",
        "            preds = model(texts)\n",
        "            loss = loss_fn(preds, labels)\n",
        "            preds = torch.argmax(torch.log_softmax(preds, dim=1), dim=1)\n",
        "            acc = accuracy(preds, labels)\n",
        "            current_loss += loss\n",
        "            current_acc += acc\n",
        "    avg_val_loss = current_loss.item() / len(validation_dataloader)\n",
        "    avg_val_acc = current_acc.item() / len(validation_dataloader)\n",
        "\n",
        "    if avg_val_acc > best_accuracy:\n",
        "        print(f'Accuracy increased [{best_accuracy:.4f} --> {avg_val_acc:.4f}]. Saving the model...')\n",
        "        best_accuracy = avg_val_acc\n",
        "        torch.save(model, 'model_best.pt')\n",
        "\n",
        "    print(f'Epoch: {i}\\tTrain loss: {avg_train_loss:.4f}\\tVal loss: {avg_val_loss:.4f}\\tVal acc: {avg_val_acc:.4f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the best model."
      ],
      "metadata": {
        "id": "foysvIb9tC7T"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "smJhIRzJY0in"
      },
      "outputs": [],
      "source": [
        "model = torch.load('model_best.pt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "24I6u_37HJz9"
      },
      "outputs": [],
      "source": [
        "model.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test the model on the test set and save all the labels and predictions."
      ],
      "metadata": {
        "id": "z88ZGIq2tG0s"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ntBuIZQLGdFs"
      },
      "outputs": [],
      "source": [
        "current_acc = 0\n",
        "current_loss = 0\n",
        "y_true = []\n",
        "y_pred = []\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    for text, label in test_dataloader:\n",
        "        label = label.to(torch.long)\n",
        "        preds = model(text)\n",
        "        loss = loss_fn(preds, label)\n",
        "        preds = torch.argmax(torch.log_softmax(preds, dim=1), dim=1)\n",
        "        acc = accuracy(preds, label)\n",
        "        current_loss += loss\n",
        "        current_acc += acc\n",
        "        y_true.append(label)\n",
        "        y_pred.append(preds)\n",
        "avg_test_loss = current_loss.item() / len(test_dataloader)\n",
        "avg_test_acc = current_acc.item() / len(test_dataloader)\n",
        "\n",
        "print(f'Test loss: {avg_test_loss:.6f}\\tTest acc: {avg_test_acc:.2%}')\n",
        "\n",
        "y_true = torch.hstack(y_true).cpu().numpy()\n",
        "y_pred = torch.hstack(y_pred).cpu().numpy()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 3. Evaluate the model (1 point)\n",
        "\n",
        "One way to better understand a multi-class model is to build a [confusion matrix](https://en.wikipedia.org/wiki/Confusion_matrix#Confusion_matrices_with_more_than_two_categories).\n",
        "\n",
        "Run the cell below and describe what you see. For example, which classes are the most difficult for the model to differentiate? What one are the easiest? Why do you think it happens?"
      ],
      "metadata": {
        "id": "QQ-fdgJvtRLR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "\n",
        "class_labels = ['very negative', 'negative', 'neutral', 'positive', 'very positive']\n",
        "ConfusionMatrixDisplay.from_predictions(y_true, y_pred, display_labels=class_labels, xticks_rotation=\"vertical\");"
      ],
      "metadata": {
        "id": "KBIuhddZjByf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "__YOUR ANSWER HERE__"
      ],
      "metadata": {
        "id": "-iVeKcZzuEtR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 4. Fine-tuned embeddings (1 point)\n",
        "\n",
        "Since we fine-tuned the pretrained word embeddings, they should now better fit our task, which is sentiment classification.\n",
        "\n",
        "Try different words that have sentimental meaning, e.g. \"bad\", \"good\", \"fantastic\", \"disgusting\", and find similar words to them using both pretrained GloVe and fine-tuned embeddings. \n",
        "\n",
        "Briefly describe which words you tried and which difference you observed. Provide some examples."
      ],
      "metadata": {
        "id": "wQefkgYCuHiN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_word_embedding(word, embeddings, vocab):\n",
        "    return embeddings[vocab[word]]\n",
        "\n",
        "def find_most_similar(word, embeddings, token2idx, idx2token, n=5):\n",
        "    word_norm = torch.nn.functional.normalize(get_word_embedding(word, embeddings, token2idx), dim=0)\n",
        "    emb_norm = torch.nn.functional.normalize(embeddings, dim=1)\n",
        "    similarities = torch.matmul(word_norm, emb_norm.T)\n",
        "    top_n = torch.argsort(similarities, descending=True)[1:n+1]\n",
        "    \n",
        "    print(f'Top-{n} most similar to the word \"{word}\":\\n')\n",
        "    for idx in top_n:\n",
        "        print(f'{idx2token[idx]}\\t{similarities[idx]:.4f}')"
      ],
      "metadata": {
        "id": "Sy2QdRBUjj2y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pretrained Glove embeddings\n",
        "find_most_similar('bad', glove_vecs, token2idx, idx2token)"
      ],
      "metadata": {
        "id": "ihKs24RinhYs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fine-tuned embeddings\n",
        "find_most_similar('bad', model.emb.weight, token2idx, idx2token)"
      ],
      "metadata": {
        "id": "CkkJkHxVnvLl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "__YOUR ANSWER HERE__"
      ],
      "metadata": {
        "id": "6rlBQN0qu-D3"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Homeword4_CNN_Release.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}